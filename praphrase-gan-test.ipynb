{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb97d6e",
   "metadata": {
    "papermill": {
     "duration": 0.003318,
     "end_time": "2025-08-01T15:55:34.888451",
     "exception": false,
     "start_time": "2025-08-01T15:55:34.885133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad928781",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-01T15:55:34.894467Z",
     "iopub.status.busy": "2025-08-01T15:55:34.894171Z",
     "iopub.status.idle": "2025-08-01T15:56:01.509769Z",
     "shell.execute_reply": "2025-08-01T15:56:01.509124Z"
    },
    "papermill": {
     "duration": 26.62005,
     "end_time": "2025-08-01T15:56:01.511157",
     "exception": false,
     "start_time": "2025-08-01T15:55:34.891107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (3.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from h5py) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->h5py) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->h5py) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.3->h5py) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.3->h5py) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.3->h5py) (2024.2.0)\r\n",
      "Collecting tensorboardX\r\n",
      "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (1.26.4)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (25.0)\r\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (3.20.3)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->tensorboardX) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->tensorboardX) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->tensorboardX) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->tensorboardX) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->tensorboardX) (2024.2.0)\r\n",
      "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tensorboardX\r\n",
      "Successfully installed tensorboardX-2.6.4\r\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py\n",
    "!pip install tensorboardX\n",
    "import json\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29595667",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T15:56:01.518149Z",
     "iopub.status.busy": "2025-08-01T15:56:01.517790Z",
     "iopub.status.idle": "2025-08-01T15:56:12.147625Z",
     "shell.execute_reply": "2025-08-01T15:56:12.146610Z"
    },
    "papermill": {
     "duration": 10.634745,
     "end_time": "2025-08-01T15:56:12.148865",
     "exception": false,
     "start_time": "2025-08-01T15:56:01.514120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /kaggle/input/qqp-processed/quora_data_prepro.json\n",
      "Loading HDF5 from /kaggle/input/qqp-processed/quora_data_prepro.h5\n",
      "Training samples: 100000\n",
      "Test samples: 30000\n",
      "ques shape: torch.Size([130000, 28])\n",
      "label shape: torch.Size([130000, 28])\n",
      "len shape: torch.Size([130000])\n",
      "label_len shape: torch.Size([130000])\n",
      "id shape: torch.Size([130000])\n",
      "ques: torch.Size([64, 28])\n",
      "ques_len: torch.Size([64])\n",
      "label: torch.Size([64, 28])\n",
      "label_len: torch.Size([64])\n",
      "ids: torch.Size([64])\n",
      "vocab size: 27699\n",
      "seq length: 28\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 1) Imports\n",
    "import json\n",
    "import h5py\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "# %%\n",
    "# 2) Dataset Class\n",
    "class Dataloader(data.Dataset):\n",
    "    def __init__(self, input_json_file_path, input_ques_h5_path):\n",
    "        super(Dataloader, self).__init__()\n",
    "        \n",
    "        print('Reading', input_json_file_path)\n",
    "        # Load JSON vocab map\n",
    "        with open(input_json_file_path) as input_file:\n",
    "            data_dict = json.load(input_file)\n",
    "        \n",
    "        # Build ix_to_word (int → str)\n",
    "        self.ix_to_word = {}\n",
    "        for k, w in data_dict['ix_to_word'].items():\n",
    "            self.ix_to_word[int(k)] = w\n",
    "        \n",
    "        # Add special tokens\n",
    "        self.UNK_token = 0\n",
    "        if 0 not in self.ix_to_word:\n",
    "            self.ix_to_word[0] = '<UNK>'\n",
    "        else:\n",
    "            raise Exception(\"Index 0 already exists in ix_to_word\")\n",
    "        \n",
    "        self.EOS_token = len(self.ix_to_word)\n",
    "        self.ix_to_word[self.EOS_token] = '<EOS>'\n",
    "        self.PAD_token = len(self.ix_to_word)\n",
    "        self.ix_to_word[self.PAD_token] = '<PAD>'\n",
    "        self.SOS_token = len(self.ix_to_word)\n",
    "        self.ix_to_word[self.SOS_token] = '<SOS>'\n",
    "        self.vocab_size = len(self.ix_to_word)\n",
    "        \n",
    "        # Load HDF5\n",
    "        print('Loading HDF5 from', input_ques_h5_path)\n",
    "        qa_data = h5py.File(input_ques_h5_path, 'r')\n",
    "        \n",
    "        # Train split\n",
    "        ques_train_raw = torch.from_numpy(qa_data['ques_train'][...].astype('int64'))\n",
    "        ques_len_train = torch.from_numpy(qa_data['ques_length_train'][...].astype('int64'))\n",
    "        label_train_raw = torch.from_numpy(qa_data['ques1_train'][...].astype('int64'))\n",
    "        label_len_train = torch.from_numpy(qa_data['ques1_length_train'][...].astype('int64'))\n",
    "        self.train_id = torch.from_numpy(qa_data['ques_cap_id_train'][...].astype('int64'))\n",
    "        print('Training samples:', ques_train_raw.size(0))\n",
    "        \n",
    "        # Test split\n",
    "        ques_test_raw = torch.from_numpy(qa_data['ques_test'][...].astype('int64'))\n",
    "        ques_len_test = torch.from_numpy(qa_data['ques_length_test'][...].astype('int64'))\n",
    "        label_test_raw = torch.from_numpy(qa_data['ques1_test'][...].astype('int64'))\n",
    "        label_len_test = torch.from_numpy(qa_data['ques1_length_test'][...].astype('int64'))\n",
    "        self.test_id = torch.from_numpy(qa_data['ques_cap_id_test'][...].astype('int64'))\n",
    "        print('Test samples:', ques_test_raw.size(0))\n",
    "        \n",
    "        qa_data.close()\n",
    "        \n",
    "        # Process + add SOS/EOS/PAD\n",
    "        q_tr, ql_tr = self.process_data(ques_train_raw, ques_len_train)\n",
    "        l_tr, ll_tr = self.process_data(label_train_raw, label_len_train)\n",
    "        q_ts, ql_ts = self.process_data(ques_test_raw, ques_len_test)\n",
    "        l_ts, ll_ts = self.process_data(label_test_raw, label_len_test)\n",
    "        \n",
    "        # Concatenate splits\n",
    "        self.ques = torch.cat([q_tr, q_ts], dim=0)\n",
    "        self.len = torch.cat([ql_tr, ql_ts], dim=0)\n",
    "        self.label = torch.cat([l_tr, l_ts], dim=0)\n",
    "        self.label_len = torch.cat([ll_tr, ll_ts], dim=0)\n",
    "        self.id = torch.cat([self.train_id, self.test_id], dim=0)\n",
    "        \n",
    "        # Verify and truncate to ensure consistent sizes\n",
    "        print('ques shape:', self.ques.shape)\n",
    "        print('label shape:', self.label.shape)\n",
    "        print('len shape:', self.len.shape)\n",
    "        print('label_len shape:', self.label_len.shape)\n",
    "        print('id shape:', self.id.shape)\n",
    "        min_size = min(self.ques.size(0), self.label.size(0), self.len.size(0), \n",
    "                      self.label_len.size(0), self.id.size(0))\n",
    "        if min_size < self.ques.size(0):\n",
    "            print(f\"Truncating to {min_size} samples to ensure consistency\")\n",
    "            self.ques = self.ques[:min_size]\n",
    "            self.label = self.label[:min_size]\n",
    "            self.len = self.len[:min_size]\n",
    "            self.label_len = self.label_len[:min_size]\n",
    "            self.id = self.id[:min_size]\n",
    "        \n",
    "        self.seq_length = self.ques.size(1)\n",
    "        # Verify sequence length consistency\n",
    "        if self.ques.size(1) != self.label.size(1):\n",
    "            raise ValueError(f\"Sequence length mismatch: ques={self.ques.size(1)}, label={self.label.size(1)}\")\n",
    "    \n",
    "    def process_data(self, data, data_len):\n",
    "        N, L = data.size()\n",
    "        new_L = L + 2  # for SOS and EOS\n",
    "        new_data = torch.full((N, new_L), fill_value=self.PAD_token, dtype=torch.long)\n",
    "        new_len = data_len.clone() + 2\n",
    "        for i in range(N):\n",
    "            l = data_len[i]\n",
    "            # SOS\n",
    "            new_data[i, 0] = self.SOS_token\n",
    "            # Copy tokens\n",
    "            new_data[i, 1:1+l] = data[i, :l]\n",
    "            # EOS\n",
    "            new_data[i, 1+l] = self.EOS_token\n",
    "        return new_data, new_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len.size(0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.ques[idx],       # LongTensor [seq_len]\n",
    "            self.len[idx],        # LongTensor [1]\n",
    "            self.label[idx],      # LongTensor [seq_len]\n",
    "            self.label_len[idx],  # LongTensor [1]\n",
    "            self.id[idx]          # LongTensor [1]\n",
    "        )\n",
    "    \n",
    "    def getVocabSize(self):\n",
    "        return self.vocab_size\n",
    "    \n",
    "    def getSeqLength(self):\n",
    "        return self.seq_length\n",
    "\n",
    "# %%\n",
    "# 3) Instantiate & wrap in DataLoader\n",
    "JSON_PATH = '/kaggle/input/qqp-processed/quora_data_prepro.json'\n",
    "H5_PATH = '/kaggle/input/qqp-processed/quora_data_prepro.h5'\n",
    "\n",
    "dataset = Dataloader(JSON_PATH, H5_PATH)\n",
    "loader = data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2, drop_last=True)\n",
    "\n",
    "# %%\n",
    "# 4) Quick sanity check\n",
    "batch = next(iter(loader))\n",
    "ques, ques_len, label, label_len, ids = batch\n",
    "print('ques:', ques.shape)\n",
    "print('ques_len:', ques_len.shape)\n",
    "print('label:', label.shape)\n",
    "print('label_len:', label_len.shape)\n",
    "print('ids:', ids.shape)\n",
    "print('vocab size:', dataset.getVocabSize())\n",
    "print('seq length:', dataset.getSeqLength())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "585fc64a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T15:56:12.155912Z",
     "iopub.status.busy": "2025-08-01T15:56:12.155655Z",
     "iopub.status.idle": "2025-08-01T15:56:12.176016Z",
     "shell.execute_reply": "2025-08-01T15:56:12.175446Z"
    },
    "papermill": {
     "duration": 0.025007,
     "end_time": "2025-08-01T15:56:12.177080",
     "exception": false,
     "start_time": "2025-08-01T15:56:12.152073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 26) (100000, 26)\n",
      "(30000, 26) (30000, 26)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "with h5py.File(H5_PATH, 'r') as f:\n",
    "    print(f['ques_train'].shape, f['ques1_train'].shape)\n",
    "    print(f['ques_test'].shape, f['ques1_test'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42125116",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T15:56:12.183854Z",
     "iopub.status.busy": "2025-08-01T15:56:12.183417Z",
     "iopub.status.idle": "2025-08-01T15:56:12.191267Z",
     "shell.execute_reply": "2025-08-01T15:56:12.190424Z"
    },
    "papermill": {
     "duration": 0.01238,
     "end_time": "2025-08-01T15:56:12.192373",
     "exception": false,
     "start_time": "2025-08-01T15:56:12.179993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 1) Utils: one_hot, prob2pred, decode_sequence, JointEmbeddingLoss\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "\n",
    "def one_hot(t, c):\n",
    "    \"\"\"\n",
    "    t: LongTensor of shape [seq_len, batch_size]\n",
    "    c: vocab size\n",
    "    returns: FloatTensor [seq_len, batch_size, c]\n",
    "    \"\"\"\n",
    "    return torch.zeros(*t.size(), c, device=t.device) \\\n",
    "                .scatter_(-1, t.unsqueeze(-1), 1.0)\n",
    "\n",
    "def prob2pred(prob):\n",
    "    \"\"\"\n",
    "    prob: Log‐prob Tensor [seq_len, batch]\n",
    "    returns: LongTensor sampled indices [seq_len, batch]\n",
    "    \"\"\"\n",
    "    # flatten to [seq_len*batch, vocab], sample, then reshape\n",
    "    v = torch.exp(prob.view(-1, prob.size(-1)))\n",
    "    samp = torch.multinomial(v, 1)\n",
    "    return samp.view(prob.size(0), prob.size(1))\n",
    "\n",
    "def decode_sequence(ix_to_word, seq):\n",
    "    \"\"\"\n",
    "    ix_to_word: dict { idx: token_str }\n",
    "    seq: LongTensor [batch, seq_len]\n",
    "    returns: list of decoded strings (batch)\n",
    "    \"\"\"\n",
    "    decoded = []\n",
    "    B, L = seq.size()\n",
    "    for i in range(B):\n",
    "        words = []\n",
    "        for j in range(L):\n",
    "            ix = int(seq[i, j].item())\n",
    "            w = ix_to_word.get(ix, '<UNK>')\n",
    "            if w == '<EOS>':\n",
    "                words.append(w)\n",
    "                break\n",
    "            if w not in ('<PAD>', '<SOS>'):\n",
    "                words.append(w)\n",
    "        decoded.append(\" \".join(words))\n",
    "    return decoded\n",
    "\n",
    "def JointEmbeddingLoss(emb1, emb2, margin=1.0):\n",
    "    \"\"\"\n",
    "    emb1, emb2: FloatTensor [batch, emb_dim]\n",
    "    returns: scalar margin loss averaged over batch^2 pairs\n",
    "    \"\"\"\n",
    "    # similarity matrix\n",
    "    sims = emb1 @ emb2.t()          # [B, B]\n",
    "    pos  = sims.diag().unsqueeze(1) # [B,1]\n",
    "    # margin loss\n",
    "    raw  = sims - pos + margin     # [B, B]\n",
    "    raw.fill_diagonal_(0)           # zero out i==j\n",
    "    return torch.clamp(raw, min=0).sum() / (emb1.size(0)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3147bf2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T15:56:12.199185Z",
     "iopub.status.busy": "2025-08-01T15:56:12.198649Z",
     "iopub.status.idle": "2025-08-01T15:56:12.207954Z",
     "shell.execute_reply": "2025-08-01T15:56:12.207241Z"
    },
    "papermill": {
     "duration": 0.013749,
     "end_time": "2025-08-01T15:56:12.208949",
     "exception": false,
     "start_time": "2025-08-01T15:56:12.195200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 2) Model: ParaphraseGenerator (combines encoder, generator, discriminator branches)\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ParaphraseGenerator(nn.Module):\n",
    "    def __init__(self, op):\n",
    "        super().__init__()\n",
    "        # shared encoder/discriminator\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.Linear(op['vocab_sz'], op['emb_hid_dim']),\n",
    "            nn.Threshold(1e-6, 0),\n",
    "            nn.Linear(op['emb_hid_dim'], op['emb_dim']),\n",
    "            nn.Threshold(1e-6, 0)\n",
    "        )\n",
    "        self.enc_rnn = nn.GRU(op['emb_dim'], op['enc_rnn_dim'])\n",
    "        self.enc_lin = nn.Sequential(\n",
    "            nn.Dropout(op['enc_dropout']),\n",
    "            nn.Linear(op['enc_rnn_dim'], op['enc_dim'])\n",
    "        )\n",
    "        # generator\n",
    "        self.gen_emb = nn.Embedding(op['vocab_sz'], op['emb_dim'])\n",
    "        self.gen_rnn = nn.LSTM(op['enc_dim'], op['gen_rnn_dim'])\n",
    "        self.gen_lin = nn.Sequential(\n",
    "            nn.Dropout(op['gen_dropout']),\n",
    "            nn.Linear(op['gen_rnn_dim'], op['vocab_sz']),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "        self.max_seq_len = op['max_seq_len']\n",
    "        self.vocab_sz    = op['vocab_sz']\n",
    "\n",
    "    def forward(self, phrase, sim_phrase=None, train=False):\n",
    "        # phrase & sim_phrase: [T, B]\n",
    "        if sim_phrase is None:\n",
    "            sim_phrase = phrase\n",
    "        # encode\n",
    "        one_in = one_hot(phrase, self.vocab_sz)\n",
    "        _, h = self.enc_rnn(self.emb_layer(one_in))\n",
    "        enc = self.enc_lin(h)         # [1, B, enc_dim]\n",
    "        # generate\n",
    "        if train:\n",
    "            emb_tgt = self.gen_emb(sim_phrase)\n",
    "            inp_seq = torch.cat([enc, emb_tgt[:-1]], dim=0)\n",
    "            out_rnn, _ = self.gen_rnn(inp_seq)\n",
    "            out = self.gen_lin(out_rnn)\n",
    "            # embeddings for loss\n",
    "            emb_gt  = self.enc_lin(self.enc_rnn(self.emb_layer(one_hot(sim_phrase, self.vocab_sz)))[1])\n",
    "            emb_gen = self.enc_lin(self.enc_rnn(self.emb_layer(torch.exp(out)))[1])\n",
    "        else:\n",
    "            words, h0 = [], None\n",
    "            inp = enc\n",
    "            for _ in range(self.max_seq_len):\n",
    "                out_rnn, h0 = self.gen_rnn(inp, hx=h0)\n",
    "                logits = self.gen_lin(out_rnn)\n",
    "                words.append(logits)\n",
    "                sampled = prob2pred(logits)\n",
    "                inp = self.gen_emb(sampled)\n",
    "            out = torch.cat(words, dim=0)\n",
    "            emb_gt  = self.enc_lin(self.enc_rnn(self.emb_layer(one_hot(sim_phrase, self.vocab_sz)))[1])\n",
    "            emb_gen = self.enc_lin(self.enc_rnn(self.emb_layer(torch.exp(out)))[1])\n",
    "        return out, emb_gen.squeeze(0), emb_gt.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20a4ad7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T15:56:12.215054Z",
     "iopub.status.busy": "2025-08-01T15:56:12.214852Z",
     "iopub.status.idle": "2025-08-01T15:56:12.219075Z",
     "shell.execute_reply": "2025-08-01T15:56:12.218324Z"
    },
    "papermill": {
     "duration": 0.008486,
     "end_time": "2025-08-01T15:56:12.220179",
     "exception": false,
     "start_time": "2025-08-01T15:56:12.211693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# token-level cross-entropy\n",
    "ce_loss = nn.CrossEntropyLoss(ignore_index=dataset.PAD_token)\n",
    "\n",
    "# pair-wise discriminator margin loss\n",
    "def JointEmbeddingLoss(emb1, emb2, margin=1.0):\n",
    "    sims = emb1 @ emb2.t()\n",
    "    pos  = sims.diag().unsqueeze(1)\n",
    "    raw  = sims - pos + margin\n",
    "    raw.fill_diagonal_(0)\n",
    "    return torch.clamp(raw, min=0).sum() / (emb1.size(0)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a580fbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T15:56:12.226799Z",
     "iopub.status.busy": "2025-08-01T15:56:12.226615Z",
     "iopub.status.idle": "2025-08-01T15:56:12.247661Z",
     "shell.execute_reply": "2025-08-01T15:56:12.247127Z"
    },
    "papermill": {
     "duration": 0.025681,
     "end_time": "2025-08-01T15:56:12.248642",
     "exception": false,
     "start_time": "2025-08-01T15:56:12.222961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def vec_norm(seq, n=4):\n",
    "    tokens = seq.split()\n",
    "    ngrams = [{} for _ in range(n)]\n",
    "    norms = [0.0] * n\n",
    "    for i in range(n):\n",
    "        for j in range(len(tokens) - i):\n",
    "            ngram = tuple(tokens[j:j+i+1])\n",
    "            ngrams[i][ngram] = ngrams[i].get(ngram, 0) + 1\n",
    "            norms[i] += ngrams[i][ngram]\n",
    "        norms[i] = norms[i] ** 0.5 if norms[i] > 0 else 1.0\n",
    "    return ngrams, norms\n",
    "\n",
    "def get_ngrams(seq, n):\n",
    "    tokens = seq.split()\n",
    "    return Counter(tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1))\n",
    "\n",
    "# BLEU\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def get_ngrams(seq, n):\n",
    "    \"\"\"\n",
    "    Extract n-grams from a sequence of tokens.\n",
    "    \n",
    "    Args:\n",
    "        seq (str): Space-separated sequence of tokens.\n",
    "        n (int): N-gram order.\n",
    "    \n",
    "    Returns:\n",
    "        Counter: Dictionary of n-grams and their counts.\n",
    "    \"\"\"\n",
    "    tokens = seq.split()\n",
    "    return Counter(tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1))\n",
    "\n",
    "class BleuScorer:\n",
    "    def __init__(self, n=4):\n",
    "        self.n = n\n",
    "        self.hyp = []\n",
    "        self.refs = []\n",
    "    \n",
    "    def add_instance(self, hyp, refs):\n",
    "        \"\"\"\n",
    "        Add a hypothesis and its reference(s).\n",
    "        \n",
    "        Args:\n",
    "            hyp (str): Hypothesis sequence.\n",
    "            refs (list): List of reference sequences.\n",
    "        \"\"\"\n",
    "        self.hyp.append(hyp)\n",
    "        self.refs.append(refs)\n",
    "    \n",
    "    def compute_score(self):\n",
    "        \"\"\"\n",
    "        Compute BLEU scores for n-grams (1 to n).\n",
    "        \n",
    "        Returns:\n",
    "            scores (list): BLEU scores for each n-gram order.\n",
    "            None: Placeholder for compatibility.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for n in range(1, self.n + 1):\n",
    "            total_precision = 0.0\n",
    "            total_brevity_penalty = 0.0\n",
    "            count = 0\n",
    "            \n",
    "            for hyp, refs in zip(self.hyp, self.refs):\n",
    "                hyp_ngrams = get_ngrams(hyp, n)\n",
    "                hyp_len = max(len(hyp.split()) - n + 1, 1)\n",
    "                \n",
    "                # Compute clipped counts\n",
    "                ref_ngrams_list = [get_ngrams(ref, n) for ref in refs]\n",
    "                ref_counts = Counter()\n",
    "                for ref_ngrams in ref_ngrams_list:\n",
    "                    for ng in ref_ngrams:\n",
    "                        ref_counts[ng] = max(ref_counts[ng], ref_ngrams[ng])\n",
    "                \n",
    "                clipped_count = sum(min(hyp_ngrams[ng], ref_counts.get(ng, 0)) for ng in hyp_ngrams)\n",
    "                precision = clipped_count / hyp_len if hyp_len > 0 else 0.0\n",
    "                \n",
    "                # Brevity penalty\n",
    "                hyp_words = len(hyp.split())\n",
    "                ref_lengths = [len(ref.split()) for ref in refs]\n",
    "                closest_ref_len = min(ref_lengths, key=lambda x: abs(x - hyp_words))\n",
    "                brevity_penalty = 1.0 if hyp_words >= closest_ref_len else math.exp(1 - closest_ref_len / hyp_words if hyp_words > 0 else 0)\n",
    "                \n",
    "                total_precision += precision\n",
    "                total_brevity_penalty += brevity_penalty\n",
    "                count += 1\n",
    "            \n",
    "            avg_precision = total_precision / count if count > 0 else 0.0\n",
    "            avg_brevity_penalty = total_brevity_penalty / count if count > 0 else 1.0\n",
    "            bleu_n = avg_precision * avg_brevity_penalty if avg_precision > 0 else 0.0\n",
    "            scores.append(bleu_n)\n",
    "        \n",
    "        return scores, None\n",
    "\n",
    "class Bleu:\n",
    "    def __init__(self, n=4):\n",
    "        self.n = n\n",
    "        self.scorer = BleuScorer(n=n)\n",
    "    \n",
    "    def compute_score(self, gts, res):\n",
    "        \"\"\"\n",
    "        Compute BLEU score for ground truth and predicted sequences.\n",
    "        \n",
    "        Args:\n",
    "            gts (dict): {id: [ref_str, ...]} mapping IDs to lists of reference strings.\n",
    "            res (dict): {id: [hyp_str]} mapping IDs to lists of hypothesis strings.\n",
    "        \n",
    "        Returns:\n",
    "            scores (list): BLEU scores for n-grams (1 to n).\n",
    "            None: Placeholder for compatibility.\n",
    "        \"\"\"\n",
    "        for id in gts:\n",
    "            self.scorer.add_instance(res[id][0], gts[id])\n",
    "        return self.scorer.compute_score()\n",
    "\n",
    "# CIDEr\n",
    "def vec_norm(seq, n=4):\n",
    "    \"\"\"\n",
    "    Convert a sequence (string) into a list of n-gram count dictionaries and their norms.\n",
    "    \n",
    "    Args:\n",
    "        seq (str): Space-separated sequence of tokens (e.g., \"how to learn python\").\n",
    "        n (int): Maximum n-gram order (e.g., 4 for 1- to 4-grams).\n",
    "    \n",
    "    Returns:\n",
    "        ngrams (list): List of dictionaries, where ngrams[i] maps (i+1)-grams to counts.\n",
    "        norms (list): List of norms for each n-gram order.\n",
    "    \"\"\"\n",
    "    # Split string into tokens\n",
    "    tokens = seq.split()\n",
    "    ngrams = [{} for _ in range(n)]\n",
    "    norms = [0.0] * n\n",
    "    \n",
    "    # Generate n-grams for each order (1 to n)\n",
    "    for i in range(n):\n",
    "        for j in range(len(tokens) - i):\n",
    "            ngram = tuple(tokens[j:j+i+1])\n",
    "            ngrams[i][ngram] = ngrams[i].get(ngram, 0) + 1\n",
    "            norms[i] += ngrams[i][ngram]\n",
    "        norms[i] = norms[i] ** 0.5 if norms[i] > 0 else 1.0\n",
    "    \n",
    "    return ngrams, norms\n",
    "\n",
    "class CiderScorer:\n",
    "    def __init__(self, n=4):\n",
    "        self.n = n\n",
    "        self.hyp = []\n",
    "        self.refs = []\n",
    "    \n",
    "    def add_instance(self, hyp, refs):\n",
    "        \"\"\"\n",
    "        Add a hypothesis and its reference(s) to the scorer.\n",
    "        \n",
    "        Args:\n",
    "            hyp (str): Hypothesis sequence (e.g., \"how to study python\").\n",
    "            refs (list): List of reference sequences (e.g., [\"how to learn python\"]).\n",
    "        \"\"\"\n",
    "        self.hyp.append(hyp)\n",
    "        self.refs.append(refs)\n",
    "    \n",
    "    def compute_score(self):\n",
    "        \"\"\"\n",
    "        Compute CIDEr scores for all n-grams up to self.n.\n",
    "        \n",
    "        Returns:\n",
    "            scores (list): CIDEr scores for n-grams (1 to n).\n",
    "            None: Placeholder for compatibility.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for i in range(self.n):\n",
    "            num = 0.0\n",
    "            for hyp, refs in zip(self.hyp, self.refs):\n",
    "                vh, nh = vec_norm(hyp, i+1)\n",
    "                for ref in refs:\n",
    "                    vr, nr = vec_norm(ref, i+1)\n",
    "                    num += sum(min(vh[i].get(ng, 0), vr[i].get(ng, 0)) * vr[i].get(ng, 0)\n",
    "                               for ng in set(vh[i]) | set(vr[i]))\n",
    "                    den = nh[i] * nr[i] if nh[i] and nr[i] else 1.0\n",
    "                    num /= den\n",
    "            scores.append(num / len(self.hyp) if self.hyp else 0.0)\n",
    "        return scores, None\n",
    "\n",
    "class Cider:\n",
    "    def __init__(self, n=4):\n",
    "        self.n = n\n",
    "        self.scorer = CiderScorer(n=n)\n",
    "    \n",
    "    def compute_score(self, gts, res):\n",
    "        \"\"\"\n",
    "        Compute CIDEr score for ground truth (gts) and predicted (res) sequences.\n",
    "        \n",
    "        Args:\n",
    "            gts (dict): {id: [ref_str, ...]} mapping IDs to lists of reference strings.\n",
    "            res (dict): {id: [hyp_str]} mapping IDs to lists of hypothesis strings.\n",
    "        \n",
    "        Returns:\n",
    "            score (float): Aggregated CIDEr score.\n",
    "            scores (list): CIDEr scores for each n-gram order.\n",
    "        \"\"\"\n",
    "        for id in gts:\n",
    "            self.scorer.add_instance(res[id][0], gts[id])\n",
    "        scores, _ = self.scorer.compute_score()\n",
    "        return sum(scores) / len(scores) if scores else 0.0, scores\n",
    "  \n",
    "\n",
    "# ROUGE-L\n",
    "class Rouge:\n",
    "    def __init__(self,beta=1.2): self.beta=beta\n",
    "    def _lcs(self,x,y):\n",
    "        m,n=len(x),len(y); dp=[[0]*(n+1) for _ in range(m+1)]\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                dp[i+1][j+1] = dp[i][j]+1 if x[i]==y[j] else max(dp[i][j+1],dp[i+1][j])\n",
    "        return dp[m][n]\n",
    "    def compute_score(self,gts,res):\n",
    "        scores=[]\n",
    "        for id in gts:\n",
    "            h=res[id][0].split(); refs=[r.split() for r in gts[id]]\n",
    "            prec=[self._lcs(r,h)/len(h) for r in refs]\n",
    "            rec=[self._lcs(r,h)/len(r) for r in refs]\n",
    "            pmax, rmax = max(prec), max(rec)\n",
    "            scores.append((1+self.beta**2)*pmax*rmax/(rmax+self.beta**2*pmax) if pmax and rmax else 0.0)\n",
    "        return float(np.mean(scores)), np.array(scores)\n",
    "class RougeL(Rouge): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14218284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T15:56:12.255201Z",
     "iopub.status.busy": "2025-08-01T15:56:12.254992Z",
     "iopub.status.idle": "2025-08-01T16:17:05.221496Z",
     "shell.execute_reply": "2025-08-01T16:17:05.220549Z"
    },
    "papermill": {
     "duration": 1252.97377,
     "end_time": "2025-08-01T16:17:05.225183",
     "exception": false,
     "start_time": "2025-08-01T15:56:12.251413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 15:56:14.048622: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754063774.242245      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754063774.305983      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /kaggle/input/qqp-processed/quora_data_prepro.json\n",
      "Loading HDF5 from /kaggle/input/qqp-processed/quora_data_prepro.h5\n",
      "Training samples: 100000\n",
      "Test samples: 30000\n",
      "ques shape: torch.Size([130000, 28])\n",
      "label shape: torch.Size([130000, 28])\n",
      "len shape: torch.Size([130000])\n",
      "label_len shape: torch.Size([130000])\n",
      "id shape: torch.Size([130000])\n",
      "Epoch 1 | Train L1=3.5065 L2=0.1389 | Val L1=6.9555 L2=4.1730 | BLEU-4=0.0009 CIDEr=0.0000 ROUGE-L=0.2781\n",
      "Epoch 2 | Train L1=2.9125 L2=0.0244 | Val L1=6.9303 L2=5.5830 | BLEU-4=0.0021 CIDEr=0.0000 ROUGE-L=0.3184\n",
      "Epoch 3 | Train L1=2.6513 L2=0.0147 | Val L1=7.0239 L2=5.6058 | BLEU-4=0.0043 CIDEr=0.0000 ROUGE-L=0.3337\n",
      "Epoch 4 | Train L1=2.4657 L2=0.0108 | Val L1=7.0324 L2=6.0703 | BLEU-4=0.0084 CIDEr=0.0000 ROUGE-L=0.3589\n",
      "Epoch 5 | Train L1=2.3307 L2=0.0085 | Val L1=7.0412 L2=6.1699 | BLEU-4=0.0116 CIDEr=0.0000 ROUGE-L=0.3721\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "JSON_PATH = '/kaggle/input/qqp-processed/quora_data_prepro.json'\n",
    "H5_PATH = '/kaggle/input/qqp-processed/quora_data_prepro.h5'\n",
    "LOG_DIR = '/kaggle/working/logs'\n",
    "SAVE_DIR = '/kaggle/working/save'\n",
    "SAMPLE_DIR = '/kaggle/working/samples'\n",
    "\n",
    "for d in (LOG_DIR, SAVE_DIR, SAMPLE_DIR): os.makedirs(d, exist_ok=True)\n",
    "\n",
    "TIME = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparams\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "TRAIN_SIZE = 100000\n",
    "VAL_SIZE = 30000\n",
    "LR = 1e-3\n",
    "\n",
    "# Data\n",
    "dataset = Dataloader(JSON_PATH, H5_PATH)\n",
    "train_ds = Subset(dataset, range(TRAIN_SIZE))\n",
    "val_ds = Subset(dataset, range(TRAIN_SIZE, TRAIN_SIZE+VAL_SIZE))\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "# Model & optimizer\n",
    "op = {\"vocab_sz\": dataset.vocab_size,\n",
    "      \"emb_hid_dim\": 256, \"emb_dim\": 512,\n",
    "      \"enc_rnn_dim\": 512, \"enc_dim\": 512, \"enc_dropout\": 0.5,\n",
    "      \"gen_rnn_dim\": 512, \"gen_dropout\": 0.5,\n",
    "      \"max_seq_len\": dataset.len.max().item()}\n",
    "model = ParaphraseGenerator(op).to(DEVICE)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=LR)\n",
    "ce_loss = nn.CrossEntropyLoss(ignore_index=dataset.PAD_token)\n",
    "\n",
    "# Logger\n",
    "writer = SummaryWriter(os.path.join(LOG_DIR, TIME))\n",
    "\n",
    "def dump_samples(ins, gts, preds, fname):\n",
    "    with open(fname, 'w') as f:\n",
    "        for i, g, p in zip(ins, gts, preds):\n",
    "            f.write(f\"IN : {i}\\nGT : {g}\\nPR : {p}\\n---\\n\")\n",
    "\n",
    "# Training\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    # Train\n",
    "    model.train()\n",
    "    sum_l1 = 0\n",
    "    sum_l2 = 0\n",
    "    cnt = 0\n",
    "    all_in, all_gt, all_pr = [], [], []\n",
    "    for inp, il, lbl, ll, ids in train_loader:\n",
    "        inp, lbl = inp.t().to(DEVICE), lbl.t().to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out, eg, et = model(inp, sim_phrase=lbl, train=True)\n",
    "        l1 = ce_loss(out.permute(1,2,0), lbl.t())\n",
    "        l2 = JointEmbeddingLoss(eg, et)\n",
    "        (l1+l2).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        sum_l1 += l1.item()\n",
    "        sum_l2 += l2.item()\n",
    "        cnt += 1\n",
    "        all_in += decode_sequence(dataset.ix_to_word, inp.t().cpu())\n",
    "        all_gt += decode_sequence(dataset.ix_to_word, lbl.t().cpu())\n",
    "        all_pr += decode_sequence(dataset.ix_to_word, torch.argmax(out, dim=-1).t().cpu())\n",
    "    writer.add_scalar('L1/train', sum_l1/cnt, epoch)\n",
    "    writer.add_scalar('L2/train', sum_l2/cnt, epoch)\n",
    "    dump_samples(all_in[:5], all_gt[:5], all_pr[:5], f\"{SAMPLE_DIR}/{TIME}_train{epoch}.txt\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    v1 = 0\n",
    "    v2 = 0\n",
    "    vc = 0\n",
    "    vin, vgt, vpr = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for inp, il, lbl, ll, ids in val_loader:\n",
    "            inp, lbl = inp.t().to(DEVICE), lbl.t().to(DEVICE)\n",
    "            out, eg, et = model(inp, sim_phrase=lbl, train=False)\n",
    "            l1 = ce_loss(out.permute(1,2,0), lbl.t())\n",
    "            l2 = JointEmbeddingLoss(eg, et)\n",
    "            v1 += l1.item()\n",
    "            v2 += l2.item()\n",
    "            vc += 1\n",
    "            vin += decode_sequence(dataset.ix_to_word, inp.t().cpu())\n",
    "            vgt += decode_sequence(dataset.ix_to_word, lbl.t().cpu())\n",
    "            vpr += decode_sequence(dataset.ix_to_word, torch.argmax(out, dim=-1).t().cpu())\n",
    "    writer.add_scalar('L1/val', v1/vc, epoch)\n",
    "    writer.add_scalar('L2/val', v2/vc, epoch)\n",
    "\n",
    "    # Metrics calculation\n",
    "    bleu = Bleu(n=4)\n",
    "    cider = Cider(n=4)\n",
    "    rouge = RougeL()\n",
    "    gts = {i: [gt] for i, gt in enumerate(vgt)}\n",
    "    res = {i: [pr] for i, pr in enumerate(vpr)}\n",
    "    gts = {i: [gt if isinstance(gt, str) else ' '.join(gt)] for i, gt in gts.items()}\n",
    "    res = {i: [pr if isinstance(pr, str) else ' '.join(pr)] for i, pr in res.items()}\n",
    "    bleu_score, _ = bleu.compute_score(gts, res)\n",
    "    cider_score, _ = cider.compute_score(gts, res)\n",
    "    rouge_score = rouge.compute_score(gts, res)[0]\n",
    "    writer.add_scalar('BLEU/val', bleu_score[-1], epoch)\n",
    "    writer.add_scalar('CIDEr/val', cider_score, epoch)\n",
    "    writer.add_scalar('ROUGE-L/val', rouge_score, epoch)\n",
    "    dump_samples(vin[:5], vgt[:5], vpr[:5], f\"{SAMPLE_DIR}/{TIME}_val{epoch}.txt\")\n",
    "\n",
    "    # Checkpoint\n",
    "    os.makedirs(f\"{SAVE_DIR}/{TIME}\", exist_ok=True)\n",
    "    torch.save({'epoch': epoch, 'model': model.state_dict(), 'opt': optimizer.state_dict()},\n",
    "               f\"{SAVE_DIR}/{TIME}/epoch{epoch}.pt\")\n",
    "    print(f\"Epoch {epoch} | Train L1={sum_l1/cnt:.4f} L2={sum_l2/cnt:.4f} | Val L1={v1/vc:.4f} L2={v2/vc:.4f} | BLEU-4={bleu_score[-1]:.4f} CIDEr={cider_score:.4f} ROUGE-L={rouge_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd59f8f5",
   "metadata": {
    "papermill": {
     "duration": 0.002875,
     "end_time": "2025-08-01T16:17:05.231246",
     "exception": false,
     "start_time": "2025-08-01T16:17:05.228371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 747,
     "sourceId": 1423,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7983962,
     "sourceId": 12634958,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1297.696871,
   "end_time": "2025-08-01T16:17:08.490445",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-01T15:55:30.793574",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
