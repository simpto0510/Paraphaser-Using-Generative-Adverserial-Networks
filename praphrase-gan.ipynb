{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1423,"sourceType":"datasetVersion","datasetId":747},{"sourceId":12634958,"sourceType":"datasetVersion","datasetId":7983962}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataloader\n","metadata":{}},{"cell_type":"code","source":"!pip install h5py\nimport json\nimport h5py\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-31T21:11:15.437930Z","iopub.execute_input":"2025-07-31T21:11:15.438202Z","iopub.status.idle":"2025-07-31T21:11:19.620831Z","shell.execute_reply.started":"2025-07-31T21:11:15.438178Z","shell.execute_reply":"2025-07-31T21:11:19.619943Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (3.14.0)\nRequirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from h5py) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->h5py) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->h5py) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.3->h5py) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.3->h5py) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.3->h5py) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ---------------- 1. SET-UP & IMPORTS ----------------\n!pip install h5py tqdm --quiet   # tiny helper libs\n\nimport pandas as pd, re, json, h5py, numpy as np\nfrom collections import Counter\nfrom tqdm import tqdm\ntqdm.pandas()\n\nCSV_PATH   = \"/kaggle/input/question-pairs-dataset/questions.csv\"          # ← your upload\nOUTPUT_DIR = \"/kaggle/working/qqp-prepro/\"          # files will appear here\nMAX_SRC    = 30   # max tokens per source Q\nMAX_TGT    = 30   # max tokens per target Q\nMIN_FREQ   = 2    # drop singletons from vocab\n\nPAD,  SOS, EOS = \"<pad>\", \"<sos>\", \"<eos>\"\n\n# ---------------- 2. LOAD & BASIC CLEAN ----------------\ndf = pd.read_csv(CSV_PATH)\nprint(df.head(3))\n\n# Keep only rows with both Q1 & Q2 text and (optionally) duplicate label==1\ndf = df.dropna(subset=[\"question1\", \"question2\"])\n# If you want only paraphrase pairs, uncomment:\n# df = df[df[\"is_duplicate\"] == 1]\n\ndf = df[[\"question1\", \"question2\"]].reset_index(drop=True)\nprint(\"Total usable pairs:\", len(df))\n\n# ---------------- 3. SIMPLE TEXT NORMALISATION ----------------\ndef clean(text):\n    text = str(text).lower().strip()\n    text = re.sub(r\"[^a-z0-9?.,' ]+\", \" \", text)   # keep basic chars\n    text = re.sub(r\"\\s+\", \" \", text)               # collapse spaces\n    return text\n\ndf[\"q1\"] = df[\"question1\"].progress_apply(clean)\ndf[\"q2\"] = df[\"question2\"].progress_apply(clean)\n\n# ---------------- 4. TOKENISE ----------------\ndef tok(text): return text.split()                 # whitespace tokeniser\ndf[\"tok1\"] = df[\"q1\"].progress_apply(tok)\ndf[\"tok2\"] = df[\"q2\"].progress_apply(tok)\n\n# ---------------- 5. BUILD VOCAB ----------------\nall_tokens = [tok for row in df[[\"tok1\",\"tok2\"]].values.ravel() for tok in row]\nvocab_cnt  = Counter(all_tokens)\n# keep tokens with freq ≥ MIN_FREQ\nvocab      = [PAD, SOS, EOS] + [w for w,c in vocab_cnt.items() if c >= MIN_FREQ]\nw2idx      = {w:i for i,w in enumerate(vocab)}\nidx2word   = {i:w for w,i in w2idx.items()}\nprint(\"Vocab size:\", len(vocab))\n\n# ---------------- 6. ENCODE & PAD/TRUNCATE ----------------\ndef encode(tok_list, max_len):\n    ids = [w2idx.get(w, w2idx[PAD]) for w in tok_list][:max_len-1]\n    ids = [w2idx[SOS]] + ids                         # prepend <sos>\n    ids = ids + [w2idx[EOS]]\n    ids = ids[:max_len]                              # truncate inc. <eos>\n    pad_len = max_len - len(ids)\n    return ids + [w2idx[PAD]]*pad_len, len(ids)      # ids, true length\n\nsrc_ids, tgt_ids, src_len, tgt_len = [], [], [], []\nfor t1, t2 in tqdm(df[[\"tok1\",\"tok2\"]].values):\n    ids1, l1 = encode(t1, MAX_SRC)\n    ids2, l2 = encode(t2, MAX_TGT)\n    src_ids.append(ids1); tgt_ids.append(ids2)\n    src_len.append(l1);   tgt_len.append(l2)\n\nsrc_ids = np.array(src_ids, dtype=np.int32)\ntgt_ids = np.array(tgt_ids, dtype=np.int32)\nsrc_len = np.array(src_len, dtype=np.int32)\ntgt_len = np.array(tgt_len, dtype=np.int32)\nprint(\"Array shapes →\", src_ids.shape, tgt_ids.shape)\n\n# ---------------- 7. SAVE HDF5 ----------------\n!mkdir -p \"$OUTPUT_DIR\"\nh5_path = OUTPUT_DIR + \"quora_data_prepro.h5\"\nwith h5py.File(h5_path, \"w\") as h5f:\n    h5f.create_dataset(\"source_ids\", data=src_ids, compression=\"gzip\")\n    h5f.create_dataset(\"target_ids\", data=tgt_ids, compression=\"gzip\")\n    h5f.create_dataset(\"source_len\", data=src_len, compression=\"gzip\")\n    h5f.create_dataset(\"target_len\", data=tgt_len, compression=\"gzip\")\nprint(\"✅ HDF5 saved →\", h5_path)\n\n# ---------------- 8. SAVE VOCAB JSON ----------------\njson_path = OUTPUT_DIR + \"quora_data_prepro.json\"\nwith open(json_path, \"w\") as f:\n    json.dump({\"ix_to_word\": idx2word}, f)\nprint(\"✅ JSON saved  →\", json_path)\n\n# ---------------- 9. DONE ----------------\nprint(\"\\nFinished preprocessing!  You can now mount\",\n      \"'/kaggle/working/qqp-prepro' as input in a new notebook.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T21:11:19.622264Z","iopub.execute_input":"2025-07-31T21:11:19.622762Z","iopub.status.idle":"2025-07-31T21:11:43.885152Z","shell.execute_reply.started":"2025-07-31T21:11:19.622738Z","shell.execute_reply":"2025-07-31T21:11:43.884270Z"}},"outputs":[{"name":"stdout","text":"   id  qid1  qid2                                          question1  \\\n0   0     1     2  What is the step by step guide to invest in sh...   \n1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n2   2     5     6  How can I increase the speed of my internet co...   \n\n                                           question2  is_duplicate  \n0  What is the step by step guide to invest in sh...             0  \n1  What would happen if the Indian government sto...             0  \n2  How can Internet speed be increased by hacking...             0  \nTotal usable pairs: 404348\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 404348/404348 [00:02<00:00, 152190.41it/s]\n100%|██████████| 404348/404348 [00:02<00:00, 155039.34it/s]\n100%|██████████| 404348/404348 [00:01<00:00, 305330.42it/s]\n100%|██████████| 404348/404348 [00:01<00:00, 210205.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Vocab size: 83223\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 404348/404348 [00:05<00:00, 72674.66it/s] \n","output_type":"stream"},{"name":"stdout","text":"Array shapes → (404348, 30) (404348, 30)\n✅ HDF5 saved → /kaggle/working/qqp-prepro/quora_data_prepro.h5\n✅ JSON saved  → /kaggle/working/qqp-prepro/quora_data_prepro.json\n\nFinished preprocessing!  You can now mount '/kaggle/working/qqp-prepro' as input in a new notebook.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import json, h5py, numpy as np, torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\n# ------------------------------------------------------------------\n# 1. Paths\nDATA_DIR   = \"/kaggle/working/qqp-prepro\"        # change if different\nH5_PATH    = f\"{DATA_DIR}/quora_data_prepro.h5\"\nJSON_PATH  = f\"{DATA_DIR}/quora_data_prepro.json\"\n\n# ------------------------------------------------------------------\n# 2. Load the vocab mapping (ix_to_word) so idx→word is available\nwith open(JSON_PATH, \"r\") as f:\n    ix2word = json.load(f)[\"ix_to_word\"]          # { \"0\":\"<pad>\", \"1\":\"<sos>\", ... }\nidx2word   = [ix2word[str(i)] for i in range(len(ix2word))]\nvocab      = {w:i for i,w in enumerate(idx2word)}  # word → index\nPAD_IDX    = vocab[\"<pad>\"]\n\nprint(\"Vocab size =\", len(vocab))\n\n# ------------------------------------------------------------------\n# 3. Load pre-tokenised / padded arrays from HDF5\nwith h5py.File(H5_PATH, \"r\") as h5f:\n    source_ids = h5f[\"source_ids\"][:]     # shape (N, max_src_len)\n    target_ids = h5f[\"target_ids\"][:]     # shape (N, max_tgt_len)\n    source_len = h5f[\"source_len\"][:]     # shape (N,)\n    target_len = h5f[\"target_len\"][:]     # shape (N,)\n\nprint(\"Loaded arrays →\", source_ids.shape, target_ids.shape)\n\n# ------------------------------------------------------------------\n# 4. Torch Dataset\nclass QQPDataset(Dataset):\n    def __init__(self, src, tgt, sl, tl):\n        # store numpy arrays (efficient) – convert to tensors per sample\n        self.src, self.tgt, self.sl, self.tl = src, tgt, sl, tl\n    def __len__(self): return len(self.src)\n    def __getitem__(self, idx):\n        return {\n            \"src\":     torch.from_numpy(self.src[idx]).long(),\n            \"tgt\":     torch.from_numpy(self.tgt[idx]).long(),\n            \"src_len\": torch.tensor(self.sl[idx]).long(),\n            \"tgt_len\": torch.tensor(self.tl[idx]).long()\n        }\n\nfull_ds = QQPDataset(source_ids, target_ids, source_len, target_len)\n\n# ------------------------------------------------------------------\n# 5. Split into train / val\nVAL_FRAC   = 0.4              # 10 % validation\nval_size   = int(len(full_ds)*VAL_FRAC)\ntrain_size = len(full_ds) - val_size\ntrain_ds, val_ds = random_split(full_ds, [train_size, val_size],\n                                generator=torch.Generator().manual_seed(42))\n\nprint(f\"Train size = {len(train_ds)},  Val size = {len(val_ds)}\")\n\n# ------------------------------------------------------------------\n# 6. DataLoaders\nBATCH_SIZE = 32\nNUM_WORKERS= 4                # 0 if running into multiprocessing issues\n\ntrain_loader = DataLoader(train_ds,\n                          batch_size=BATCH_SIZE,\n                          shuffle=True,\n                          num_workers=NUM_WORKERS,\n                          drop_last=True)\n\nval_loader   = DataLoader(val_ds,\n                          batch_size=BATCH_SIZE,\n                          shuffle=False,\n                          num_workers=NUM_WORKERS)\n\n# ------------------------------------------------------------------\n# 7. Quick sanity-check – pull one batch\nbatch = next(iter(train_loader))\nprint(\"Batch shapes:\",\n      batch[\"src\"].shape,     # [B, max_src_len]\n      batch[\"tgt\"].shape,     # [B, max_tgt_len]\n      batch[\"src_len\"].shape) # [B]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T21:11:43.887177Z","iopub.execute_input":"2025-07-31T21:11:43.887412Z","iopub.status.idle":"2025-07-31T21:11:48.403926Z","shell.execute_reply.started":"2025-07-31T21:11:43.887391Z","shell.execute_reply":"2025-07-31T21:11:48.403157Z"}},"outputs":[{"name":"stdout","text":"Vocab size = 83223\nLoaded arrays → (404348, 30) (404348, 30)\nTrain size = 242609,  Val size = 161739\nBatch shapes: torch.Size([32, 30]) torch.Size([32, 30]) torch.Size([32])\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Grok\n","metadata":{}},{"cell_type":"code","source":"# import json\n# import h5py\n# import torch\n# from torch.utils.data import Dataset, DataLoader\n# import numpy as np\n\n# # 1) Load JSON and build idx2word + vocab\n# json_path = '/kaggle/input/qqp-processed/quora_data_prepro.json'\n# with open(json_path, 'r') as f:\n#     meta = json.load(f)\n\n# # ix_to_word maps string indices (1-based) to tokens\n# ix_to_word = {int(k): v for k, v in meta['ix_to_word'].items()}\n# print(f\"Original ix_to_word keys: min={min(ix_to_word.keys())}, max={max(ix_to_word.keys())}, count={len(ix_to_word)}\")\n\n# # Add <pad> and <sos> tokens\n# idx2word = ['<pad>', '<sos>']  # Indices 0 and 1\n# vocab = {'<pad>': 0, '<sos>': 1}\n# # Shift original indices up by 2\n# for idx in sorted(ix_to_word.keys()):\n#     idx2word.append(ix_to_word[idx])\n#     vocab[ix_to_word[idx]] = len(idx2word) - 1\n\n# print(f\"Loaded vocab of size {len(vocab)}\")\n\n# # 2) Load HDF5 file\n# h5_path = '/kaggle/input/qqp-processed/quora_data_prepro.h5'\n# with h5py.File(h5_path, 'r') as h5f:\n#     print(\"HDF5 contains:\", list(h5f.keys()))\n\n#     # Load training sequences and lengths\n#     src_key = 'ques1_train'\n#     tgt_key = 'ques_train'\n#     src_len_key = 'ques1_length_train'\n#     tgt_len_key = 'ques_length_train'\n\n#     if not all(k in h5f.keys() for k in [src_key, tgt_key, src_len_key, tgt_len_key]):\n#         raise KeyError(f\"Required keys {[src_key, tgt_key, src_len_key, tgt_len_key]} not found in {list(h5f.keys())}\")\n\n#     source_ids = np.array(h5f[src_key][:], dtype=np.int64)\n#     target_ids = np.array(h5f[tgt_key][:], dtype=np.int64)\n#     source_lens = np.array(h5f[src_len_key][:], dtype=np.int64)\n#     target_lens = np.array(h5f[tgt_len_key][:], dtype=np.int64)\n\n#     # Verify data shapes\n#     print(f\"Source IDs shape: {source_ids.shape}, Target IDs shape: {target_ids.shape}\")\n#     print(f\"Source lengths shape: {source_lens.shape}, Target lengths shape: {target_lens.shape}\")\n\n#     # Adjust indices: HDF5 indices (1-based) → new indices (shifted by 2)\n#     source_ids = np.where(source_ids == 0, 0, source_ids + 1)  # 0 stays <pad>, others shift\n#     target_ids = np.where(target_ids == 0, 0, target_ids + 1)\n#     # Clip to valid range\n#     source_ids = np.clip(source_ids, 0, len(idx2word) - 1)\n#     target_ids = np.clip(target_ids, 0, len(idx2word) - 1)\n\n# # 3) Define Dataset\n# class QQPDataset(Dataset):\n#     def __init__(self, src_ids, tgt_ids, src_lens, tgt_lens):\n#         self.src_ids = src_ids\n#         self.tgt_ids = tgt_ids\n#         self.src_lens = src_lens\n#         self.tgt_lens = tgt_lens\n\n#     def __len__(self):\n#         return len(self.src_ids)\n\n#     def __getitem__(self, idx):\n#         return {\n#             'src': torch.tensor(self.src_ids[idx], dtype=torch.long),\n#             'tgt': torch.tensor(self.tgt_ids[idx], dtype=torch.long),\n#             'src_len': torch.tensor(self.src_lens[idx], dtype=torch.long),\n#             'tgt_len': torch.tensor(self.tgt_lens[idx], dtype=torch.long)\n#         }\n\n# # 4) Instantiate Dataset\n# train_dataset = QQPDataset(source_ids, target_ids, source_lens, target_lens)\n\n# # 5) Create Training DataLoader\n# train_loader = DataLoader(\n#     train_dataset,\n#     batch_size=64,\n#     shuffle=True,\n#     num_workers=2,\n#     drop_last=True\n# )\n\n# # 6) Create Validation DataLoader\n# with h5py.File(h5_path, 'r') as h5f:\n#     src_key = 'ques1_test'\n#     tgt_key = 'ques_test'\n#     src_len_key = 'ques1_length_test'\n#     tgt_len_key = 'ques_length_test'\n\n#     if not all(k in h5f.keys() for k in [src_key, tgt_key, src_len_key, tgt_len_key]):\n#         raise KeyError(f\"Validation keys {[src_key, tgt_key, src_len_key, tgt_len_key]} not found in {list(h5f.keys())}\")\n\n#     val_source_ids = np.array(h5f[src_key][:], dtype=np.int64)\n#     val_target_ids = np.array(h5f[tgt_key][:], dtype=np.int64)\n#     val_source_lens = np.array(h5f[src_len_key][:], dtype=np.int64)\n#     val_target_lens = np.array(h5f[tgt_len_key][:], dtype=np.int64)\n\n#     # Adjust indices\n#     val_source_ids = np.where(val_source_ids == 0, 0, val_source_ids + 1)\n#     val_target_ids = np.where(val_target_ids == 0, 0, val_target_ids + 1)\n#     val_source_ids = np.clip(val_source_ids, 0, len(idx2word) - 1)\n#     val_target_ids = np.clip(val_target_ids, 0, len(idx2word) - 1)\n\n# val_dataset = QQPDataset(val_source_ids, val_target_ids, val_source_lens, val_target_lens)\n# val_loader = DataLoader(\n#     val_dataset,\n#     batch_size=64,\n#     shuffle=False,\n#     num_workers=2,\n#     drop_last=False\n# )\n\n# # 7) Test a batch\n# for batch in train_loader:\n#     print(\"Train batch keys:\", batch.keys())\n#     print(\"Src shape:\", batch['src'].shape)\n#     print(\"Tgt shape:\", batch['tgt'].shape)\n#     print(\"Src_len shape:\", batch['src_len'].shape)\n#     print(\"Tgt_len shape:\", batch['tgt_len'].shape)\n#     break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T21:11:48.405096Z","iopub.execute_input":"2025-07-31T21:11:48.405720Z","iopub.status.idle":"2025-07-31T21:11:48.411799Z","shell.execute_reply.started":"2025-07-31T21:11:48.405695Z","shell.execute_reply":"2025-07-31T21:11:48.411111Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# import json\n# import h5py\n\n# # 1) Load the JSON and build idx2word + vocab\n# json_path = '/kaggle/input/qqp-processed/quora_data_prepro.json'\n# with open(json_path, 'r') as f:\n#     meta = json.load(f)\n\n# # meta only has 'ix_to_word', mapping string indices → tokens\n# ix_to_word = {int(k): v for k, v in meta['ix_to_word'].items()}\n# print(f\"ix_to_word keys: min={min(ix_to_word.keys())}, max={max(ix_to_word.keys())}, count={len(ix_to_word)}\")\n\n# # Build a list so idx2word[i] = token, using sorted keys\n# sorted_keys = sorted(ix_to_word.keys())\n# idx2word = [ix_to_word[k] for k in sorted_keys]\n# # And invert to get word→idx\n# vocab = {w: i for i, w in enumerate(idx2word)}\n\n# print(f\"Loaded vocab of size {len(vocab)}\")\n\n# # 2) Inspect your HDF5 to see what datasets it contains\n# h5_path = '/kaggle/input/qqp-processed/quora_data_prepro.h5'\n# with h5py.File(h5_path, 'r') as h5f:\n#     print(\"HDF5 contains:\", list(h5f.keys()))\n\n#     # 3) Automatically pick the two arrays for source/target sequences\n#     all_keys = list(h5f.keys())\n#     src_key = next((k for k in all_keys if 'source' in k.lower()), None)\n#     tgt_key = next((k for k in all_keys if 'target' in k.lower()), None)\n\n#     if src_key is None or tgt_key is None:\n#         raise KeyError(f\"Couldn't find 'source'/'target' in {all_keys}\")\n\n#     source_ids = h5f[src_key][:]\n#     target_ids = h5f[tgt_key][:]\n\n# # 4) Derive max lengths from their shapes\n# max_src_len = source_ids.shape[1]\n# max_tgt_len = target_ids.shape[1]\n\n# print(f\"Using HDF5 keys: src='{src_key}', tgt='{tgt_key}'\")\n# print(f\"Max source length = {max_src_len},  max target length = {max_tgt_len}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T21:11:48.412641Z","iopub.execute_input":"2025-07-31T21:11:48.412883Z","iopub.status.idle":"2025-07-31T21:11:48.429126Z","shell.execute_reply.started":"2025-07-31T21:11:48.412867Z","shell.execute_reply":"2025-07-31T21:11:48.428581Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# json_path = '/kaggle/input/qqp-processed/quora_data_prepro.json'\n# with open(json_path, 'r') as f:\n#     meta = json.load(f)\n# # Extract key info\n# vocab      = meta['w2idx']        # word→index mapping\n# idx2word   = meta['idx2w']        # list of tokens by index\n# max_src_len = meta['max_src_len']\n# max_tgt_len = meta['max_tgt_len']\n\n# print(f\"Vocab size: {len(vocab)}, max lengths: {max_src_len}/{max_tgt_len}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T21:11:48.429850Z","iopub.execute_input":"2025-07-31T21:11:48.430108Z","iopub.status.idle":"2025-07-31T21:11:48.446608Z","shell.execute_reply.started":"2025-07-31T21:11:48.430091Z","shell.execute_reply":"2025-07-31T21:11:48.446071Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# h5_path = '/kaggle/input/qqp-processed/quora_data_prepro.h5'\n# with h5py.File(h5_path, 'r') as h5f:\n#     print(\"Available datasets:\", list(h5f.keys()))\n#     source_ids = h5f['source_ids'][:]   # shape (N, max_src_len)\n#     target_ids = h5f['target_ids'][:]   # shape (N, max_tgt_len)\n#     source_lens = h5f['source_len'][:]  # actual lengths\n#     target_lens = h5f['target_len'][:]\n# print(\"Loaded:\", source_ids.shape, target_ids.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T21:11:48.447381Z","iopub.execute_input":"2025-07-31T21:11:48.447601Z","iopub.status.idle":"2025-07-31T21:11:48.463448Z","shell.execute_reply.started":"2025-07-31T21:11:48.447576Z","shell.execute_reply":"2025-07-31T21:11:48.462847Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# import torch\n# from torch.utils.data import Dataset\n\n# class QQPDataset(Dataset):\n#     def __init__(self, src_ids, tgt_ids, src_lens, tgt_lens):\n#         self.src_ids   = src_ids\n#         self.tgt_ids   = tgt_ids\n#         self.src_lens  = src_lens\n#         self.tgt_lens  = tgt_lens\n\n#     def __len__(self):\n#         return len(self.src_ids)\n\n#     def __getitem__(self, idx):\n#         return {\n#             'src':     torch.tensor(self.src_ids[idx], dtype=torch.long),\n#             'tgt':     torch.tensor(self.tgt_ids[idx], dtype=torch.long),\n#             'src_len': torch.tensor(self.src_lens[idx], dtype=torch.long),\n#             'tgt_len': torch.tensor(self.tgt_lens[idx], dtype=torch.long)\n#         }\n\n# # Instantiate\n# dataset = QQPDataset(source_ids, target_ids, source_lens, target_lens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T21:11:48.464144Z","iopub.execute_input":"2025-07-31T21:11:48.464402Z","iopub.status.idle":"2025-07-31T21:11:48.475922Z","shell.execute_reply.started":"2025-07-31T21:11:48.464375Z","shell.execute_reply":"2025-07-31T21:11:48.475362Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# from torch.utils.data import DataLoader\n\n# loader = DataLoader(\n#     dataset,\n#     batch_size=64,\n#     shuffle=True,\n#     num_workers=2,\n#     drop_last=True\n# )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T21:11:48.478219Z","iopub.execute_input":"2025-07-31T21:11:48.478405Z","iopub.status.idle":"2025-07-31T21:11:48.489310Z","shell.execute_reply.started":"2025-07-31T21:11:48.478391Z","shell.execute_reply":"2025-07-31T21:11:48.488760Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Generator","metadata":{}},{"cell_type":"markdown","source":"### Original","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n\n# class ParaphraseGenerator(nn.Module):\n#     def __init__(self,\n#                  vocab_size: int,\n#                  emb_dim: int,\n#                  enc_hidden: int,\n#                  dec_hidden: int,\n#                  latent_dim: int,\n#                  max_tgt_len: int,\n#                  pad_idx: int,\n#                  dropout: float = 0.1):\n#         \"\"\"\n#         Args:\n#             vocab_size: size of the embedding vocabulary\n#             emb_dim: dimensionality of word embeddings\n#             enc_hidden: hidden size of the encoder GRU\n#             dec_hidden: hidden size of the decoder GRU\n#             latent_dim: size of the latent vector z\n#             max_tgt_len: maximum target sequence length\n#             pad_idx: padding token index\n#             dropout: dropout probability\n#         \"\"\"\n#         super().__init__()\n#         # Embedding layer\n#         self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n#         # Encoder: bidirectional GRU\n#         self.encoder = nn.GRU(emb_dim,\n#                               enc_hidden,\n#                               batch_first=True,\n#                               bidirectional=True)\n#         # Project encoder’s bidirectional hidden to decoder hidden\n#         self.enc_to_dec = nn.Linear(enc_hidden * 2 + latent_dim, dec_hidden)\n#         # Decoder: unidirectional GRU\n#         self.decoder = nn.GRU(emb_dim,\n#                               dec_hidden,\n#                               batch_first=True)\n#         # Output projection to vocabulary\n#         self.out_proj = nn.Linear(dec_hidden, vocab_size)\n#         self.dropout = nn.Dropout(dropout)\n#         self.max_tgt_len = max_tgt_len\n#         self.latent_dim = latent_dim\n\n#     def forward(self,\n#                 src_ids: torch.Tensor,\n#                 src_lens: torch.Tensor,\n#                 tgt_ids: torch.Tensor = None,\n#                 teacher_forcing_ratio: float = 0.5):\n#         \"\"\"\n#         Args:\n#             src_ids: [B, T_src] source token IDs\n#             src_lens: [B] actual lengths of each source sequence\n#             tgt_ids: [B, T_tgt] target token IDs (for teacher forcing)\n#             teacher_forcing_ratio: probability to use ground-truth token\n#         Returns:\n#             logits: [B, T_tgt, V] pre-softmax scores over vocab\n#         \"\"\"\n#         batch_size = src_ids.size(0)\n#         device = src_ids.device\n\n#         # 1) Embed and pack source sequences\n#         embedded_src = self.dropout(self.embedding(src_ids))\n#         packed = nn.utils.rnn.pack_padded_sequence(\n#             embedded_src, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n#         # 2) Encode\n#         enc_outputs, enc_hidden = self.encoder(packed)\n#         # enc_hidden: [2, B, enc_hidden] from bidirectional GRU\n#         # Concatenate forward & backward final states → [B, 2*enc_hidden]\n#         enc_hidden = torch.cat([enc_hidden[0], enc_hidden[1]], dim=-1)\n\n#         # 3) Sample latent code z ∼ N(0, I)\n#         z = torch.randn(batch_size, self.latent_dim, device=device)\n\n#         # 4) Initialize decoder hidden: project [enc_hidden; z] → [B, dec_hidden]\n#         dec_init = torch.tanh(self.enc_to_dec(torch.cat([enc_hidden, z], dim=1)))\n#         dec_hidden = dec_init.unsqueeze(0)  # [1, B, dec_hidden]\n\n#         # 5) Decode step-by-step with optional teacher forcing\n#         # Prepare first input token: assume <sos> token is index 1\n#         inputs = torch.full((batch_size, 1), 1, dtype=torch.long, device=device)\n#         logits = []\n\n#         for t in range(self.max_tgt_len):\n#             emb_t = self.dropout(self.embedding(inputs))  # [B, 1, emb_dim]\n#             out, dec_hidden = self.decoder(emb_t, dec_hidden)\n#             # out: [B, 1, dec_hidden]\n#             step_logits = self.out_proj(out.squeeze(1))  # [B, V]\n#             logits.append(step_logits.unsqueeze(1))       # accumulate\n\n#             # Next input: either ground-truth or greedy sample\n#             if tgt_ids is not None and torch.rand(1).item() < teacher_forcing_ratio:\n#                 inputs = tgt_ids[:, t].unsqueeze(1)  # teacher forcing\n#             else:\n#                 inputs = step_logits.argmax(dim=-1).unsqueeze(1)\n\n#         # Concatenate logits: [B, T_tgt, V]\n#         return torch.cat(logits, dim=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T21:11:48.490200Z","iopub.execute_input":"2025-07-31T21:11:48.490507Z","iopub.status.idle":"2025-07-31T21:11:48.503930Z","shell.execute_reply.started":"2025-07-31T21:11:48.490484Z","shell.execute_reply":"2025-07-31T21:11:48.503350Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Grok Mod","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ParaphraseGenerator(nn.Module):\n    def __init__(\n        self,\n        vocab_size: int,\n        emb_dim: int,\n        enc_hidden: int,\n        dec_hidden: int,\n        latent_dim: int,\n        max_tgt_len: int,\n        pad_idx: int,\n        dropout: float = 0.1\n    ):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        self.encoder = nn.GRU(emb_dim, enc_hidden, batch_first=True, bidirectional=True)\n        self.enc_to_dec = nn.Linear(enc_hidden * 2 + latent_dim, dec_hidden)\n        self.decoder = nn.GRU(emb_dim, dec_hidden, batch_first=True)\n        self.out_proj = nn.Linear(dec_hidden, vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        self.max_tgt_len = max_tgt_len\n        self.latent_dim = latent_dim\n        self.pad_idx = pad_idx\n        self.sos_idx = 1  # Hard-coded as per original assumption\n\n    def forward(\n        self,\n        src_ids: torch.Tensor,\n        src_lens: torch.Tensor,\n        tgt_ids: torch.Tensor = None,\n        z: torch.Tensor = None,\n        return_repr: bool = False,\n        tf_ratio: float = 0.5\n    ):\n        batch_size = src_ids.size(0)\n        device = src_ids.device\n\n        # 1) Embed and pack source sequences\n        embedded_src = self.dropout(self.embedding(src_ids))\n        packed = nn.utils.rnn.pack_padded_sequence(\n            embedded_src, src_lens.cpu(), batch_first=True, enforce_sorted=False\n        )\n\n        # 2) Encode\n        enc_outputs, enc_hidden = self.encoder(packed)\n        enc_hidden = torch.cat([enc_hidden[0], enc_hidden[1]], dim=-1)  # [B, 2*enc_hidden]\n\n        # 3) Use provided z or sample new one\n        if z is None:\n            z = torch.randn(batch_size, self.latent_dim, device=device)\n\n        # 4) Initialize decoder hidden\n        dec_init = torch.tanh(self.enc_to_dec(torch.cat([enc_hidden, z], dim=1)))\n        dec_hidden = dec_init.unsqueeze(0)  # [1, B, dec_hidden]\n\n        # 5) Decode\n        inputs = torch.full((batch_size, 1), self.sos_idx, dtype=torch.long, device=device)\n        logits = []\n        outputs = []\n\n        for t in range(self.max_tgt_len):\n            emb_t = self.dropout(self.embedding(inputs))  # [B, 1, emb_dim]\n            out, dec_hidden = self.decoder(emb_t, dec_hidden)  # out: [B, 1, dec_hidden]\n            step_logits = self.out_proj(out.squeeze(1))  # [B, V]\n            logits.append(step_logits.unsqueeze(1))\n            outputs.append(out.squeeze(1))  # Accumulate for repr\n\n            # Next input\n            if tgt_ids is not None and torch.rand(1).item() < tf_ratio:\n                inputs = tgt_ids[:, t].unsqueeze(1) if t < tgt_ids.size(1) else inputs\n            else:\n                inputs = step_logits.argmax(dim=-1).unsqueeze(1)\n\n        logits = torch.cat(logits, dim=1)  # [B, T_tgt, V]\n        repr_out = torch.stack(outputs, dim=1)  # [B, T_tgt, dec_hidden]\n\n        if return_repr:\n            return logits, repr_out\n        return logits\n\n# Example usage in training loop\n# logits, repr = model_G(src, src_len, z=z1, return_repr=True, tf_ratio=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T21:11:48.504607Z","iopub.execute_input":"2025-07-31T21:11:48.504875Z","iopub.status.idle":"2025-07-31T21:11:48.522463Z","shell.execute_reply.started":"2025-07-31T21:11:48.504855Z","shell.execute_reply":"2025-07-31T21:11:48.521760Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Discriminator","metadata":{}},{"cell_type":"markdown","source":"### Original","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CNNEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_filters, filter_sizes, pad_idx, dropout=0.1):\n        \"\"\"\n        CNN-based sentence encoder using multiple filter widths and max-over-time pooling.\n        References:\n          – Kim (2014): CNNs for sentence classification :contentReference[oaicite:6]{index=6}\n        \"\"\"\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n        # Convolutional filters of sizes e.g. [3,4,5] :contentReference[oaicite:7]{index=7}\n        self.convs = nn.ModuleList([\n            nn.Conv1d(in_channels=emb_dim,\n                      out_channels=num_filters,\n                      kernel_size=fs)\n            for fs in filter_sizes\n        ])\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n          x: [B, T] token IDs\n        Returns:\n          emb: [B, num_filters * len(filter_sizes)] pooled features\n        \"\"\"\n        # Embed tokens → [B, T, emb_dim], then to [B, emb_dim, T] for conv1d :contentReference[oaicite:8]{index=8}\n        emb = self.embedding(x).permute(0, 2, 1)\n        # Apply each conv → ReLU → max-over-time pooling :contentReference[oaicite:9]{index=9}\n        pooled = [\n            F.max_pool1d(F.relu(conv(emb)), conv(emb).shape[2]).squeeze(2)\n            for conv in self.convs\n        ]\n        cat = torch.cat(pooled, dim=1)\n        return self.dropout(cat)\n\nclass Discriminator(nn.Module):\n    def __init__(self, vocab_size, emb_dim, num_filters, filter_sizes, pad_idx, dropout=0.1):\n        \"\"\"\n        DivGAN Discriminator: q(x,y) = σ(w[C(x);C(y)] + b)\n        As per Cao & Wan (2020) §3.1.2 :contentReference[oaicite:10]{index=10}.\n        \"\"\"\n        super().__init__()\n        # Shared CNN encoder for source & paraphrase :contentReference[oaicite:11]{index=11}\n        self.encoder = CNNEncoder(vocab_size, emb_dim, num_filters, filter_sizes, pad_idx, dropout)\n        feat_dim = num_filters * len(filter_sizes)\n        # Linear scoring layer → binary quality score :contentReference[oaicite:12]{index=12}\n        self.fc = nn.Linear(feat_dim * 2, 1)\n\n    def forward(self, src_ids, tgt_ids):\n        \"\"\"\n        Args:\n          src_ids: [B, T_src] source token IDs\n          tgt_ids: [B, T_tgt] paraphrase token IDs\n        Returns:\n          prob: [B, 1] q(x,y) ∈ [0,1]\n        \"\"\"\n        # Encode both sentences with shared CNN :contentReference[oaicite:13]{index=13}\n        src_feat = self.encoder(src_ids)\n        tgt_feat = self.encoder(tgt_ids)\n        # Concatenate and score → sigmoid :contentReference[oaicite:14]{index=14}\n        combined = torch.cat([src_feat, tgt_feat], dim=1)\n        logit = self.fc(combined)\n        return torch.sigmoid(logit)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T21:11:48.523130Z","iopub.execute_input":"2025-07-31T21:11:48.523370Z","iopub.status.idle":"2025-07-31T21:11:48.542281Z","shell.execute_reply.started":"2025-07-31T21:11:48.523354Z","shell.execute_reply":"2025-07-31T21:11:48.541761Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Lossses","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef discriminator_loss(D_real: torch.Tensor,\n                       D_fake: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Adversarial loss for the discriminator:\n      – D_real: discriminator output for real (x, y) pairs, shape [B,1], values in (0,1)\n      – D_fake: discriminator output for generated (x, ŷ) pairs, shape [B,1]\n    Returns:\n      scalar loss = BCE(D_real, 1) + BCE(D_fake, 0)\n    \"\"\"\n    real_labels = torch.ones_like(D_real)\n    fake_labels = torch.zeros_like(D_fake)\n    loss_real = F.binary_cross_entropy(D_real, real_labels)\n    loss_fake = F.binary_cross_entropy(D_fake, fake_labels)\n    return loss_real + loss_fake\n\ndef generator_adversarial_loss(D_fake: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Adversarial loss for the generator:\n      – D_fake: discriminator output for generated (x, ŷ) pairs, shape [B,1]\n    Returns:\n      scalar loss = BCE(D_fake, 1)\n    \"\"\"\n    real_labels = torch.ones_like(D_fake)\n    return F.binary_cross_entropy(D_fake, real_labels)\n    \n## Original\n# def diversity_loss(y1_repr: torch.Tensor,\n#                    y2_repr: torch.Tensor,\n#                    z1: torch.Tensor,\n#                    z2: torch.Tensor,\n#                    lambda_div: float = 1.0,\n#                    eps: float = 1e-8) -> torch.Tensor:\n#     \"\"\"\n#     Hinge-style diversity loss (Eq.6 in DivGAN):\n#       y1_repr, y2_repr: [B, D] representation vectors of two samples ŷ^(1), ŷ^(2)\n#       z1, z2:           [B, L] latent codes used to generate them\n#       lambda_div:       slack margin λ\n#     Returns:\n#       mean over batch of max(λ − ||y1−y2||2 / (||z1−z2||2+eps), 0)\n#     \"\"\"\n#     # L2 distances\n#     dist_y = (y1_repr - y2_repr).norm(p=2, dim=1)        # [B]\n#     dist_z = (z1   - z2  ).norm(p=2, dim=1).clamp(min=eps)  # [B]\n#     # hinge penalty\n#     hinge = F.relu(lambda_div - dist_y / dist_z)\n#     return hinge.mean()\n\ndef diversity_loss(y1_repr: torch.Tensor, y2_repr: torch.Tensor, z1: torch.Tensor, z2: torch.Tensor, lambda_div: float = 1.0, eps: float = 1e-8) -> torch.Tensor:\n    \"\"\"\n    Hinge-style diversity loss (Eq.6 in DivGAN):\n      y1_repr, y2_repr: [B, T_tgt, dec_hidden] representation vectors of two samples ŷ^(1), ŷ^(2)\n      z1, z2:           [B, latent_dim] latent codes used to generate them\n      lambda_div:       slack margin λ\n      eps:              small constant to avoid division by zero\n    Returns:\n      mean over batch of max(λ − ||y1−y2||2 / (||z1−z2||2+eps), 0)\n    \"\"\"\n    # Mean pool over sequence dimension: [B, T_tgt, dec_hidden] → [B, dec_hidden]\n    y1_repr = y1_repr.mean(dim=1)\n    y2_repr = y2_repr.mean(dim=1)\n    # L2 distances\n    dist_y = (y1_repr - y2_repr).norm(p=2, dim=1)  # [B]\n    dist_z = (z1 - z2).norm(p=2, dim=1).clamp(min=eps)  # [B]\n    # Hinge penalty\n    hinge = F.relu(lambda_div - dist_y / dist_z)\n    return hinge.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T21:11:48.543033Z","iopub.execute_input":"2025-07-31T21:11:48.543308Z","iopub.status.idle":"2025-07-31T21:11:48.563270Z","shell.execute_reply.started":"2025-07-31T21:11:48.543287Z","shell.execute_reply":"2025-07-31T21:11:48.562689Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Evaluation Metrices","metadata":{}},{"cell_type":"code","source":"!pip install sacrebleu bert-score\n\nimport sacrebleu\nfrom bert_score import score as bert_score\n\ndef compute_bleu4(references, hypotheses):\n    \"\"\"\n    Compute corpus-level BLEU-4 using sacrebleu.\n    Args:\n      references: List[str] of ground-truth sentences.\n      hypotheses: List[str] of generated sentences.\n    Returns:\n      bleu4_score: float\n    \"\"\"\n    if not references or not hypotheses:\n        return 0.0\n    # Filter out empty strings\n    refs = [r for r in references if r.strip()]\n    hyps = [h for h in hypotheses if h.strip()]\n    if not refs or not hyps:\n        return 0.0\n    # Wrap references for sacrebleu\n    refs = [refs]\n    bleu = sacrebleu.corpus_bleu(hyps, refs)\n    return bleu.score  # BLEU score in percentage\n\n\n# def compute_bertscore(references, hypotheses, lang='en', rescale_with_baseline=True):\n#     \"\"\"\n#     Compute average BERTScore F1 across the corpus.\n#     Args:\n#       references: List[str] of ground-truth sentences.\n#       hypotheses: List[str] of generated sentences.\n#       lang: language for BERTScore ('en' for English).\n#       rescale_with_baseline: whether to apply the authors’ baseline rescaling.\n#     Returns:\n#       P, R, F1: floats (precision, recall, F1)\n#     \"\"\"\n#     P, R, F1 = bert_score(hypotheses, references, lang=lang, \n#                            rescale_with_baseline=rescale_with_baseline)\n#     # bert_score returns torch tensors; convert to floats\n#     return P.mean().item(), R.mean().item(), F1.mean().item()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T21:11:48.564029Z","iopub.execute_input":"2025-07-31T21:11:48.564490Z"}},"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m898.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.52.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.4)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert-score)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.33.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.0.9)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.6.15)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert-score) (1.1.5)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: portalocker, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sacrebleu, bert-score\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"markdown","source":"### Original","metadata":{}},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from torch.utils.data import DataLoader\n# from torch.optim import Adam\n\n# # --- 1. Hyperparameters & Setup ---\n# device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# vocab_size  = len(vocab)         # from your quora_data_prepro.json\n# emb_dim     = 300\n# enc_hidden  = 256\n# dec_hidden  = 256\n# latent_dim  = 64\n# max_tgt_len = max_tgt_len        # from your metadata\n# pad_idx     = vocab['<pad>']     # or whatever pad token index is\n# num_filters = 100\n# filter_sizes= [3,4,5]\n# dropout     = 0.1\n# γ_div       = 1.0                # weight for diversity loss\n# lr_G        = 1e-4\n# lr_D        = 1e-4\n# num_epochs  = 20\n# batch_size  = 64\n\n# # --- 2. DataLoader (you’ve already done this) ---\n# dataset = QQPDataset(source_ids, target_ids, source_lens, target_lens)\n# loader  = DataLoader(dataset,\n#                      batch_size=batch_size,\n#                      shuffle=True,\n#                      num_workers=2,\n#                      drop_last=True)\n\n# # --- 3. Model Instantiation ---\n# model_G = ParaphraseGenerator(vocab_size, emb_dim,\n#                               enc_hidden, dec_hidden,\n#                               latent_dim, max_tgt_len,\n#                               pad_idx, dropout).to(device)\n# model_D = Discriminator(vocab_size, emb_dim,\n#                         num_filters, filter_sizes,\n#                         pad_idx, dropout).to(device)\n\n# # --- 4. Optimizers ---\n# opt_G = Adam(model_G.parameters(), lr=lr_G, betas=(0.5, 0.999))\n# opt_D = Adam(model_D.parameters(), lr=lr_D, betas=(0.5, 0.999))\n\n# # --- 5. Training Loop ---\n# for epoch in range(1, num_epochs+1):\n#     model_G.train()\n#     model_D.train()\n#     total_D_loss = 0.0\n#     total_G_loss = 0.0\n\n#     for batch in loader:\n#         # Move to device\n#         src     = batch['src'].to(device)      # [B, T_src]\n#         tgt     = batch['tgt'].to(device)      # [B, T_tgt]\n#         src_len = batch['src_len'].to(device)\n#         tgt_len = batch['tgt_len'].to(device)\n\n#         B = src.size(0)\n\n#         # --- 5.1 Generate two samples for diversity ---\n#         z1 = torch.randn(B, latent_dim, device=device)\n#         z2 = torch.randn(B, latent_dim, device=device)\n\n#         # Modify your generator to return (logits, repr)\n#         logits1, repr1 = model_G(src, src_len, z=z1, return_repr=True)\n#         logits2, repr2 = model_G(src, src_len, z=z2, return_repr=True)\n\n#         # Greedy decode for discriminator input\n#         pred1 = logits1.argmax(dim=-1)\n#         pred2 = logits2.argmax(dim=-1)\n\n#         # --- 5.2 Discriminator update ---\n#         opt_D.zero_grad()\n#         D_real = model_D(src, tgt)\n#         D_fake = model_D(src, pred1.detach())\n#         loss_D = discriminator_loss(D_real, D_fake)\n#         loss_D.backward()\n#         opt_D.step()\n#         total_D_loss += loss_D.item()\n\n#         # --- 5.3 Generator update ---\n#         opt_G.zero_grad()\n#         # Fool the discriminator\n#         D_fake_for_G = model_D(src, pred1)\n#         loss_G_adv   = generator_adversarial_loss(D_fake_for_G)\n#         # Diversity penalty\n#         loss_div     = diversity_loss(repr1, repr2, z1, z2, lambda_div=1.0)\n#         # (Optional) you could add a teacher-forcing MLE loss here\n#         loss_G       = loss_G_adv + γ_div * loss_div\n#         loss_G.backward()\n#         opt_G.step()\n#         total_G_loss += loss_G.item()\n\n#     avg_D = total_D_loss / len(loader)\n#     avg_G = total_G_loss / len(loader)\n#     print(f\"Epoch {epoch}/{num_epochs} → D_loss: {avg_D:.4f}, G_loss: {avg_G:.4f}\")\n\n#     # --- 5.4 Validation & Metrics (per epoch) ---\n#     model_G.eval()\n#     hyps, refs = [], []\n#     with torch.no_grad():\n#         for batch in val_loader:  # assume you created a val_loader similarly\n#             src     = batch['src'].to(device)\n#             src_len = batch['src_len'].to(device)\n#             logits  = model_G(src, src_len, z=None, teacher_forcing_ratio=0.0)\n#             preds   = logits.argmax(dim=-1).cpu().tolist()\n#             targets = batch['tgt'].cpu().tolist()\n\n#             # Convert IDs → words, strip pads\n#             for p, t in zip(preds, targets):\n#                 hyps.append(\" \".join(idx2word[idx] for idx in p if idx!=pad_idx))\n#                 refs.append(\" \".join(idx2word[idx] for idx in t if idx!=pad_idx))\n\n#     bleu4 = compute_bleu4(refs, hyps)\n#     _, _, bert_f1 = compute_bertscore(refs, hyps)\n#     print(f\"  → Val BLEU-4: {bleu4:.2f}, BERTScore F1: {bert_f1:.4f}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Number of epochs to train\n# NUM_EPOCHS = 10\n\n# for epoch in range(1, NUM_EPOCHS + 1):\n#     # --- Training ---\n#     G.train(); D.train()\n#     total_D_loss, total_G_loss = 0.0, 0.0\n\n#     for batch in loader:\n#         src, tgt = batch['src'].to(DEVICE), batch['tgt'].to(DEVICE)\n#         src_len, tgt_len = batch['src_len'].to(DEVICE), batch['tgt_len'].to(DEVICE)\n#         B = src.size(0)\n\n#         # Sample two latent codes for diversity\n#         z1 = torch.randn(B, LATENT_DIM, device=DEVICE)\n#         z2 = torch.randn(B, LATENT_DIM, device=DEVICE)\n\n#         # Generator forward + repr\n#         logits1, repr1 = G(src, src_len, z=z1, return_repr=True)\n#         logits2, repr2 = G(src, src_len, z=z2, return_repr=True)\n#         fake = logits1.argmax(dim=-1)\n\n#         # Discriminator update\n#         optD.zero_grad()\n#         D_real = D(src, tgt)\n#         D_fake = D(src, fake.detach())\n#         loss_D = discriminator_loss(D_real, D_fake)\n#         loss_D.backward()\n#         optD.step()\n#         total_D_loss += loss_D.item()\n\n#         # Generator update\n#         optG.zero_grad()\n#         Df = D(src, fake)\n#         loss_G_adv = generator_adversarial_loss(Df)\n#         loss_div  = diversity_loss(repr1, repr2, z1, z2)\n#         loss_G    = loss_G_adv + GAMMA_DIV * loss_div\n#         loss_G.backward()\n#         optG.step()\n#         total_G_loss += loss_G.item()\n\n#     avg_D = total_D_loss / len(loader)\n#     avg_G = total_G_loss / len(loader)\n#     print(f\"Epoch {epoch}/{NUM_EPOCHS} — Train D_loss: {avg_D:.4f}, G_loss: {avg_G:.4f}\")\n\n#     # --- Validation ---\n#     G.eval()\n#     all_refs, all_hyps = [], []\n\n#     with torch.no_grad():\n#         for batch in val_loader:\n#             src = batch['src'].to(DEVICE)\n#             src_len = batch['src_len'].to(DEVICE)\n\n#             # Generate with greedy decoding\n#             logits = G(src, src_len, z=None, return_repr=False, tf_ratio=0.0)\n#             preds  = logits.argmax(dim=-1).cpu().tolist()\n#             targets= batch['tgt'].cpu().tolist()\n\n#             for p, t in zip(preds, targets):\n#                 # convert ID lists to strings, stripping PAD_IDX\n#                 hyp = \" \".join(idx2word[idx] for idx in p if idx != PAD_IDX)\n#                 ref = \" \".join(idx2word[idx] for idx in t if idx != PAD_IDX)\n#                 all_hyps.append(hyp)\n#                 all_refs.append(ref)\n\n#     bleu4 = compute_bleu4(all_refs, all_hyps)\n#     _, _, bert_f1 = compute_bertscore(all_refs, all_hyps)\n#     print(f\"           Val BLEU-4: {bleu4:.2f}, BERTScore F1: {bert_f1:.4f}\")\n\n# print(\"✅ Training complete!\") \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Grok Mod","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.optim import Adam\nimport sacrebleu\nfrom bert_score import score as bert_score\n\n# Loss functions\ndef discriminator_loss(D_real: torch.Tensor, D_fake: torch.Tensor, label_smoothing: float = 0.1) -> torch.Tensor:\n    real_labels = torch.full_like(D_real, 1.0 - label_smoothing)\n    fake_labels = torch.full_like(D_fake, label_smoothing)\n    loss_real = F.binary_cross_entropy(D_real, real_labels)\n    loss_fake = F.binary_cross_entropy(D_fake, fake_labels)\n    return loss_real + loss_fake\n\ndef generator_adversarial_loss(D_fake: torch.Tensor) -> torch.Tensor:\n    real_labels = torch.ones_like(D_fake)\n    return F.binary_cross_entropy(D_fake, real_labels)\n\ndef diversity_loss(y1_repr: torch.Tensor, y2_repr: torch.Tensor, z1: torch.Tensor, z2: torch.Tensor, lambda_div: float = 1.0, eps: float = 1e-8) -> torch.Tensor:\n    y1_repr = y1_repr.mean(dim=1)\n    y2_repr = y2_repr.mean(dim=1)\n    dist_y = (y1_repr - y2_repr).norm(p=2, dim=1)\n    dist_z = (z1 - z2).norm(p=2, dim=1).clamp(min=eps)\n    hinge = F.relu(lambda_div - dist_y / dist_z)\n    return hinge.mean()\n\n# # Metrics\n# def compute_bleu4(references, hypotheses):\n#     if not references or not hypotheses:\n#         return 0.0\n#     refs = [r for r in references if r.strip()]\n#     hyps = [h for h in hypotheses if h.strip()]\n#     if not refs or not hyps:\n#         return 0.0\n#     refs = [refs]\n#     bleu = sacrebleu.corpus_bleu(hyps, refs)\n#     return bleu.score\n\n# def compute_bertscore(references, hypotheses, lang='en', rescale_with_baseline=True):\n#     P, R, F1 = bert_score(hypotheses, references, lang=lang, rescale_with_baseline=rescale_with_baseline)\n#     return P.mean().item(), R.mean().item(), F1.mean().item()\n\n# Hyperparameters & Setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nvocab_size = len(vocab)  # 27697\nemb_dim = 300\nenc_hidden = 256\ndec_hidden = 256\nlatent_dim = 64\nmax_tgt_len = 25  # Will adjust after inspection\npad_idx = vocab['<pad>']  # 0\nnum_filters = 100\nfilter_sizes = [3, 4, 5]\ndropout = 0.1\ngamma_div = 1.0\nlr_G = 1e-4\nlr_D = 1e-4\nnum_epochs = 10\n\n# Inspect data (add to data loading code or run separately)\n# print(f\"Max target length: {target_ids.shape[1]}\")\n# print(f\"Source IDs min: {source_ids.min()}, max: {source_ids.max()}\")\n# print(f\"Target IDs min: {target_ids.min()}, max: {target_ids.max()}\")\n\n# Models\nmodel_G = ParaphraseGenerator(\n    vocab_size, emb_dim, enc_hidden, dec_hidden, latent_dim, max_tgt_len, pad_idx, dropout\n).to(device)\nmodel_D = Discriminator(\n    vocab_size, emb_dim, num_filters, filter_sizes, pad_idx, dropout\n).to(device)\n\n# Optimizers\nopt_G = Adam(model_G.parameters(), lr=lr_G, betas=(0.5, 0.999))\nopt_D = Adam(model_D.parameters(), lr=lr_D, betas=(0.5, 0.999))\n\n# Training Loop\nprint(\"Training Starting\")\nfor epoch in range(1, num_epochs + 1):\n    model_G.train()\n    model_D.train()\n    total_D_loss = 0.0\n    total_G_loss = 0.0\n\n    for batch_idx, batch in enumerate(train_loader):\n        src = batch['src'].to(device)\n        tgt = batch['tgt'].to(device)\n        src_len = batch['src_len'].to(device)\n        tgt_len = batch['tgt_len'].to(device)\n        B = src.size(0)\n\n        z1 = torch.randn(B, latent_dim, device=device)\n        z2 = torch.randn(B, latent_dim, device=device)\n\n        # Generator forward (no teacher forcing for GAN steps)\n        logits1, repr1 = model_G(src, src_len, z=z1, return_repr=True, tf_ratio=0.0)\n        logits2, repr2 = model_G(src, src_len, z=z2, return_repr=True, tf_ratio=0.0)\n        pred1 = logits1.argmax(dim=-1)\n        pred2 = logits2.argmax(dim=-1)\n\n\n        # Debug: Print sample predictions\n        if batch_idx == 0 and epoch == 1:\n            sample_pred = pred1[0].cpu().tolist()\n            sample_tgt = tgt[0].cpu().tolist()\n            print(\"Sample pred:\", \" \".join(idx2word[idx] for idx in sample_pred if idx != pad_idx))\n            print(\"Sample ref:\", \" \".join(idx2word[idx] for idx in sample_tgt if idx != pad_idx))\n\n        opt_D.zero_grad()\n        D_real = model_D(src, tgt)\n        D_fake = model_D(src, pred1.detach())\n        loss_D = discriminator_loss(D_real, D_fake, label_smoothing=0.1)\n        loss_D.backward()\n        torch.nn.utils.clip_grad_norm_(model_D.parameters(), max_norm=1.0)\n        opt_D.step()\n        total_D_loss += loss_D.item()\n\n        opt_G.zero_grad()\n        D_fake_for_G = model_D(src, pred1)\n        loss_G_adv = generator_adversarial_loss(D_fake_for_G)\n        loss_div = diversity_loss(repr1, repr2, z1, z2, lambda_div=1.0)\n        loss_G = loss_G_adv + gamma_div * loss_div\n        loss_G.backward()\n        torch.nn.utils.clip_grad_norm_(model_G.parameters(), max_norm=1.0)\n        opt_G.step()\n        total_G_loss += loss_G.item()\n\n    avg_D = total_D_loss / len(train_loader)\n    avg_G = total_G_loss / len(train_loader)\n    print(f\"Epoch {epoch}/{num_epochs} → Train D_loss: {avg_D:.4f}, G_loss: {avg_G:.4f}\")\n\n    model_G.eval()\n    all_refs, all_hyps = [], []\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            src     = batch['src'].to(device)\n            src_len = batch['src_len'].to(device)\n    \n            # Greedy decode – no teacher forcing, no latent z (sample internally)\n            logits = model_G(src, src_len, z=None, return_repr=False, tf_ratio=0.0)\n            preds  = logits.argmax(dim=-1).cpu().tolist()\n            targets= batch['tgt'].cpu().tolist()\n    \n            for p, t in zip(preds, targets):\n                hyp = \" \".join(idx2word[idx] for idx in p if idx != pad_idx)\n                ref = \" \".join(idx2word[idx] for idx in t if idx != pad_idx)\n                all_hyps.append(hyp)\n                all_refs.append(ref)\n\n# BLEU-4 only\nbleu4 = compute_bleu4(all_refs, all_hyps)\nprint(f\"  → Val BLEU-4: {bleu4:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}