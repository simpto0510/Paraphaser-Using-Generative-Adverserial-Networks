{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ee4ba7",
   "metadata": {
    "papermill": {
     "duration": 0.004469,
     "end_time": "2025-08-01T17:07:53.337861",
     "exception": false,
     "start_time": "2025-08-01T17:07:53.333392",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90f07b91",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-01T17:07:53.347627Z",
     "iopub.status.busy": "2025-08-01T17:07:53.347315Z",
     "iopub.status.idle": "2025-08-01T17:08:24.356875Z",
     "shell.execute_reply": "2025-08-01T17:08:24.355958Z"
    },
    "papermill": {
     "duration": 31.016293,
     "end_time": "2025-08-01T17:08:24.358577",
     "exception": false,
     "start_time": "2025-08-01T17:07:53.342284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (3.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from h5py) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.3->h5py) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->h5py) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.3->h5py) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.3->h5py) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.3->h5py) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.3->h5py) (2024.2.0)\r\n",
      "Collecting tensorboardX\r\n",
      "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (1.26.4)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (25.0)\r\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (3.20.3)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->tensorboardX) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->tensorboardX) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->tensorboardX) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->tensorboardX) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->tensorboardX) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->tensorboardX) (2024.2.0)\r\n",
      "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tensorboardX\r\n",
      "Successfully installed tensorboardX-2.6.4\r\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py\n",
    "!pip install tensorboardX\n",
    "import json\n",
    "import h5py\n",
    "import os\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cf41af0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T17:08:24.366819Z",
     "iopub.status.busy": "2025-08-01T17:08:24.366331Z",
     "iopub.status.idle": "2025-08-01T17:08:36.350117Z",
     "shell.execute_reply": "2025-08-01T17:08:36.348910Z"
    },
    "papermill": {
     "duration": 11.98929,
     "end_time": "2025-08-01T17:08:36.351490",
     "exception": false,
     "start_time": "2025-08-01T17:08:24.362200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /kaggle/input/qqp-processed/quora_data_prepro.json\n",
      "Loading HDF5 from /kaggle/input/qqp-processed/quora_data_prepro.h5\n",
      "Training samples: 100000\n",
      "Test samples: 30000\n",
      "ques shape: torch.Size([130000, 28])\n",
      "label shape: torch.Size([130000, 28])\n",
      "len shape: torch.Size([130000])\n",
      "label_len shape: torch.Size([130000])\n",
      "id shape: torch.Size([130000])\n",
      "ques: torch.Size([64, 28])\n",
      "ques_len: torch.Size([64])\n",
      "label: torch.Size([64, 28])\n",
      "label_len: torch.Size([64])\n",
      "ids: torch.Size([64])\n",
      "vocab size: 27699\n",
      "seq length: 28\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 1) Imports\n",
    "import json\n",
    "import h5py\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "# %%\n",
    "# 2) Dataset Class\n",
    "class Dataloader(data.Dataset):\n",
    "    def __init__(self, input_json_file_path, input_ques_h5_path):\n",
    "        super(Dataloader, self).__init__()\n",
    "        \n",
    "        print('Reading', input_json_file_path)\n",
    "        # Load JSON vocab map\n",
    "        with open(input_json_file_path) as input_file:\n",
    "            data_dict = json.load(input_file)\n",
    "        \n",
    "        # Build ix_to_word (int → str)\n",
    "        self.ix_to_word = {}\n",
    "        for k, w in data_dict['ix_to_word'].items():\n",
    "            self.ix_to_word[int(k)] = w\n",
    "        \n",
    "        # Add special tokens\n",
    "        self.UNK_token = 0\n",
    "        if 0 not in self.ix_to_word:\n",
    "            self.ix_to_word[0] = '<UNK>'\n",
    "        else:\n",
    "            raise Exception(\"Index 0 already exists in ix_to_word\")\n",
    "        \n",
    "        self.EOS_token = len(self.ix_to_word)\n",
    "        self.ix_to_word[self.EOS_token] = '<EOS>'\n",
    "        self.PAD_token = len(self.ix_to_word)\n",
    "        self.ix_to_word[self.PAD_token] = '<PAD>'\n",
    "        self.SOS_token = len(self.ix_to_word)\n",
    "        self.ix_to_word[self.SOS_token] = '<SOS>'\n",
    "        self.vocab_size = len(self.ix_to_word)\n",
    "        \n",
    "        # Load HDF5\n",
    "        print('Loading HDF5 from', input_ques_h5_path)\n",
    "        qa_data = h5py.File(input_ques_h5_path, 'r')\n",
    "        \n",
    "        # Train split\n",
    "        ques_train_raw = torch.from_numpy(qa_data['ques_train'][...].astype('int64'))\n",
    "        ques_len_train = torch.from_numpy(qa_data['ques_length_train'][...].astype('int64'))\n",
    "        label_train_raw = torch.from_numpy(qa_data['ques1_train'][...].astype('int64'))\n",
    "        label_len_train = torch.from_numpy(qa_data['ques1_length_train'][...].astype('int64'))\n",
    "        self.train_id = torch.from_numpy(qa_data['ques_cap_id_train'][...].astype('int64'))\n",
    "        print('Training samples:', ques_train_raw.size(0))\n",
    "        \n",
    "        # Test split\n",
    "        ques_test_raw = torch.from_numpy(qa_data['ques_test'][...].astype('int64'))\n",
    "        ques_len_test = torch.from_numpy(qa_data['ques_length_test'][...].astype('int64'))\n",
    "        label_test_raw = torch.from_numpy(qa_data['ques1_test'][...].astype('int64'))\n",
    "        label_len_test = torch.from_numpy(qa_data['ques1_length_test'][...].astype('int64'))\n",
    "        self.test_id = torch.from_numpy(qa_data['ques_cap_id_test'][...].astype('int64'))\n",
    "        print('Test samples:', ques_test_raw.size(0))\n",
    "        \n",
    "        qa_data.close()\n",
    "        \n",
    "        # Process + add SOS/EOS/PAD\n",
    "        q_tr, ql_tr = self.process_data(ques_train_raw, ques_len_train)\n",
    "        l_tr, ll_tr = self.process_data(label_train_raw, label_len_train)\n",
    "        q_ts, ql_ts = self.process_data(ques_test_raw, ques_len_test)\n",
    "        l_ts, ll_ts = self.process_data(label_test_raw, label_len_test)\n",
    "        \n",
    "        # Concatenate splits\n",
    "        self.ques = torch.cat([q_tr, q_ts], dim=0)\n",
    "        self.len = torch.cat([ql_tr, ql_ts], dim=0)\n",
    "        self.label = torch.cat([l_tr, l_ts], dim=0)\n",
    "        self.label_len = torch.cat([ll_tr, ll_ts], dim=0)\n",
    "        self.id = torch.cat([self.train_id, self.test_id], dim=0)\n",
    "        \n",
    "        # Verify and truncate to ensure consistent sizes\n",
    "        print('ques shape:', self.ques.shape)\n",
    "        print('label shape:', self.label.shape)\n",
    "        print('len shape:', self.len.shape)\n",
    "        print('label_len shape:', self.label_len.shape)\n",
    "        print('id shape:', self.id.shape)\n",
    "        min_size = min(self.ques.size(0), self.label.size(0), self.len.size(0), \n",
    "                      self.label_len.size(0), self.id.size(0))\n",
    "        if min_size < self.ques.size(0):\n",
    "            print(f\"Truncating to {min_size} samples to ensure consistency\")\n",
    "            self.ques = self.ques[:min_size]\n",
    "            self.label = self.label[:min_size]\n",
    "            self.len = self.len[:min_size]\n",
    "            self.label_len = self.label_len[:min_size]\n",
    "            self.id = self.id[:min_size]\n",
    "        \n",
    "        self.seq_length = self.ques.size(1)\n",
    "        # Verify sequence length consistency\n",
    "        if self.ques.size(1) != self.label.size(1):\n",
    "            raise ValueError(f\"Sequence length mismatch: ques={self.ques.size(1)}, label={self.label.size(1)}\")\n",
    "    \n",
    "    def process_data(self, data, data_len):\n",
    "        N, L = data.size()\n",
    "        new_L = L + 2  # for SOS and EOS\n",
    "        new_data = torch.full((N, new_L), fill_value=self.PAD_token, dtype=torch.long)\n",
    "        new_len = data_len.clone() + 2\n",
    "        for i in range(N):\n",
    "            l = data_len[i]\n",
    "            # SOS\n",
    "            new_data[i, 0] = self.SOS_token\n",
    "            # Copy tokens\n",
    "            new_data[i, 1:1+l] = data[i, :l]\n",
    "            # EOS\n",
    "            new_data[i, 1+l] = self.EOS_token\n",
    "        return new_data, new_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len.size(0)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.ques[idx],       # LongTensor [seq_len]\n",
    "            self.len[idx],        # LongTensor [1]\n",
    "            self.label[idx],      # LongTensor [seq_len]\n",
    "            self.label_len[idx],  # LongTensor [1]\n",
    "            self.id[idx]          # LongTensor [1]\n",
    "        )\n",
    "    \n",
    "    def getVocabSize(self):\n",
    "        return self.vocab_size\n",
    "    \n",
    "    def getSeqLength(self):\n",
    "        return self.seq_length\n",
    "\n",
    "# %%\n",
    "# 3) Instantiate & wrap in DataLoader\n",
    "JSON_PATH = '/kaggle/input/qqp-processed/quora_data_prepro.json'\n",
    "H5_PATH = '/kaggle/input/qqp-processed/quora_data_prepro.h5'\n",
    "\n",
    "dataset = Dataloader(JSON_PATH, H5_PATH)\n",
    "loader = data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2, drop_last=True)\n",
    "\n",
    "# %%\n",
    "# 4) Quick sanity check\n",
    "batch = next(iter(loader))\n",
    "ques, ques_len, label, label_len, ids = batch\n",
    "print('ques:', ques.shape)\n",
    "print('ques_len:', ques_len.shape)\n",
    "print('label:', label.shape)\n",
    "print('label_len:', label_len.shape)\n",
    "print('ids:', ids.shape)\n",
    "print('vocab size:', dataset.getVocabSize())\n",
    "print('seq length:', dataset.getSeqLength())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abdcabea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T17:08:36.359703Z",
     "iopub.status.busy": "2025-08-01T17:08:36.359432Z",
     "iopub.status.idle": "2025-08-01T17:08:36.383349Z",
     "shell.execute_reply": "2025-08-01T17:08:36.382601Z"
    },
    "papermill": {
     "duration": 0.029516,
     "end_time": "2025-08-01T17:08:36.384547",
     "exception": false,
     "start_time": "2025-08-01T17:08:36.355031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 26) (100000, 26)\n",
      "(30000, 26) (30000, 26)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "with h5py.File(H5_PATH, 'r') as f:\n",
    "    print(f['ques_train'].shape, f['ques1_train'].shape)\n",
    "    print(f['ques_test'].shape, f['ques1_test'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af1893cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T17:08:36.393243Z",
     "iopub.status.busy": "2025-08-01T17:08:36.392540Z",
     "iopub.status.idle": "2025-08-01T17:08:36.401205Z",
     "shell.execute_reply": "2025-08-01T17:08:36.400420Z"
    },
    "papermill": {
     "duration": 0.014134,
     "end_time": "2025-08-01T17:08:36.402401",
     "exception": false,
     "start_time": "2025-08-01T17:08:36.388267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 1) Utils: one_hot, prob2pred, decode_sequence, JointEmbeddingLoss\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "\n",
    "def one_hot(t, c):\n",
    "    \"\"\"\n",
    "    t: LongTensor of shape [seq_len, batch_size]\n",
    "    c: vocab size\n",
    "    returns: FloatTensor [seq_len, batch_size, c]\n",
    "    \"\"\"\n",
    "    return torch.zeros(*t.size(), c, device=t.device) \\\n",
    "                .scatter_(-1, t.unsqueeze(-1), 1.0)\n",
    "\n",
    "def prob2pred(prob):\n",
    "    \"\"\"\n",
    "    prob: Log‐prob Tensor [seq_len, batch]\n",
    "    returns: LongTensor sampled indices [seq_len, batch]\n",
    "    \"\"\"\n",
    "    # flatten to [seq_len*batch, vocab], sample, then reshape\n",
    "    v = torch.exp(prob.view(-1, prob.size(-1)))\n",
    "    samp = torch.multinomial(v, 1)\n",
    "    return samp.view(prob.size(0), prob.size(1))\n",
    "\n",
    "def decode_sequence(ix_to_word, seq):\n",
    "    \"\"\"\n",
    "    ix_to_word: dict { idx: token_str }\n",
    "    seq: LongTensor [batch, seq_len]\n",
    "    returns: list of decoded strings (batch)\n",
    "    \"\"\"\n",
    "    decoded = []\n",
    "    B, L = seq.size()\n",
    "    for i in range(B):\n",
    "        words = []\n",
    "        for j in range(L):\n",
    "            ix = int(seq[i, j].item())\n",
    "            w = ix_to_word.get(ix, '<UNK>')\n",
    "            if w == '<EOS>':\n",
    "                words.append(w)\n",
    "                break\n",
    "            if w not in ('<PAD>', '<SOS>'):\n",
    "                words.append(w)\n",
    "        decoded.append(\" \".join(words))\n",
    "    return decoded\n",
    "\n",
    "def JointEmbeddingLoss(emb1, emb2, margin=1.0):\n",
    "    \"\"\"\n",
    "    emb1, emb2: FloatTensor [batch, emb_dim]\n",
    "    returns: scalar margin loss averaged over batch^2 pairs\n",
    "    \"\"\"\n",
    "    # similarity matrix\n",
    "    sims = emb1 @ emb2.t()          # [B, B]\n",
    "    pos  = sims.diag().unsqueeze(1) # [B,1]\n",
    "    # margin loss\n",
    "    raw  = sims - pos + margin     # [B, B]\n",
    "    raw.fill_diagonal_(0)           # zero out i==j\n",
    "    return torch.clamp(raw, min=0).sum() / (emb1.size(0)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c623f451",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T17:08:36.410427Z",
     "iopub.status.busy": "2025-08-01T17:08:36.410105Z",
     "iopub.status.idle": "2025-08-01T17:08:36.429981Z",
     "shell.execute_reply": "2025-08-01T17:08:36.429151Z"
    },
    "papermill": {
     "duration": 0.025549,
     "end_time": "2025-08-01T17:08:36.431430",
     "exception": false,
     "start_time": "2025-08-01T17:08:36.405881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ParaphraseGenerator(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.vocab_sz = config['vocab_sz']\n",
    "        self.emb_dim = config['emb_dim']\n",
    "        self.enc_dim = config['enc_dim']\n",
    "        self.gen_rnn_dim = config['gen_rnn_dim']\n",
    "        self.max_seq_len = config['max_seq_len']\n",
    "        self.num_heads = config.get('num_heads', 4)\n",
    "        self.SOS_token = config['SOS_token'] # Assuming SOS_token is in config\n",
    "        self.EOS_token = config['EOS_token'] # Assuming EOS_token is in config\n",
    "        self.PAD_token = config['PAD_token'] # Assuming PAD_token is in config\n",
    "\n",
    "        self.emb_layer = nn.Embedding(self.vocab_sz, self.emb_dim)\n",
    "        self.enc_rnn = nn.GRU(\n",
    "            input_size=self.emb_dim,\n",
    "            hidden_size=config['enc_rnn_dim'],\n",
    "            num_layers=2,\n",
    "            dropout=config.get('enc_dropout', 0.3),\n",
    "            bidirectional=True,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.enc_lin = nn.Sequential(\n",
    "            nn.Dropout(config['enc_dropout']),\n",
    "            nn.Linear(config['enc_rnn_dim'] * 2, self.enc_dim)\n",
    "        )\n",
    "        self.gen_emb = nn.Embedding(self.vocab_sz, self.emb_dim)\n",
    "        self.gen_rnn = nn.GRU(\n",
    "            input_size=self.enc_dim + self.emb_dim,\n",
    "            hidden_size=self.gen_rnn_dim,\n",
    "            num_layers=2,\n",
    "            dropout=config.get('gen_dropout', 0.3),\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=self.gen_rnn_dim,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout=config.get('attn_dropout', 0.1),\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.gen_lin = nn.Sequential(\n",
    "            nn.Dropout(config['gen_dropout']),\n",
    "            nn.Linear(self.gen_rnn_dim, self.vocab_sz),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "        self.context_proj = nn.Linear(config['enc_rnn_dim'] * 2, self.gen_rnn_dim)\n",
    "\n",
    "    def encode(self, phrase):\n",
    "        embedded = self.emb_layer(phrase)\n",
    "        rnn_output, hidden_state = self.enc_rnn(embedded)\n",
    "        num_layers = hidden_state.size(0) // 2\n",
    "        forward_hidden = hidden_state[num_layers-1]\n",
    "        backward_hidden = hidden_state[-1]\n",
    "        combined_hidden = torch.cat([forward_hidden, backward_hidden], dim=-1)\n",
    "        final_encoding = self.enc_lin(combined_hidden)\n",
    "        final_encoding = final_encoding.unsqueeze(0)\n",
    "        return final_encoding, rnn_output\n",
    "\n",
    "    def generate(self, encoding, encoder_states, target_seq=None, train=False):\n",
    "        batch_size = encoding.size(1)\n",
    "        if train and target_seq is not None:\n",
    "            target_embedded = self.gen_emb(target_seq[:-1])\n",
    "            encoding_expanded = encoding.expand(target_embedded.size(0), -1, -1)\n",
    "            gen_input = torch.cat([encoding_expanded, target_embedded], dim=-1)\n",
    "            gen_output, _ = self.gen_rnn(gen_input)\n",
    "            encoder_states_proj = self.context_proj(encoder_states)\n",
    "            attended_output, _ = self.cross_attention(\n",
    "                query=gen_output, key=encoder_states_proj, value=encoder_states_proj\n",
    "            )\n",
    "            output = self.gen_lin(attended_output)\n",
    "        else:\n",
    "            words = []\n",
    "            hidden_state = None\n",
    "            \n",
    "            # Use the SOS token to create the initial input for the decoder\n",
    "            initial_token_id = torch.full((1, batch_size), fill_value=self.SOS_token, dtype=torch.long, device=encoding.device)\n",
    "            initial_embedded = self.gen_emb(initial_token_id)\n",
    "            gen_input = torch.cat([encoding, initial_embedded], dim=-1)\n",
    "            \n",
    "            encoder_states_proj = self.context_proj(encoder_states)\n",
    "\n",
    "            for i in range(self.max_seq_len):\n",
    "                gen_output, hidden_state = self.gen_rnn(gen_input, hidden_state)\n",
    "                attended_output, _ = self.cross_attention(\n",
    "                    query=gen_output, key=encoder_states_proj, value=encoder_states_proj\n",
    "                )\n",
    "                word_logits = self.gen_lin(attended_output)\n",
    "                words.append(word_logits)\n",
    "                \n",
    "                # Check for EOS token to stop generation\n",
    "                predicted_token = self.prob2pred(word_logits)\n",
    "                if (predicted_token == self.EOS_token).all():\n",
    "                    break\n",
    "                \n",
    "                # Prepare the input for the next step\n",
    "                next_embedded = self.gen_emb(predicted_token)\n",
    "                gen_input = torch.cat([encoding, next_embedded], dim=-1)\n",
    "\n",
    "            output = torch.cat(words, dim=0) if words else encoding.new_zeros(1, batch_size, self.vocab_sz)\n",
    "        return output\n",
    "\n",
    "    def forward(self, phrase, sim_phrase=None, train=False):\n",
    "        if sim_phrase is None:\n",
    "            sim_phrase = phrase\n",
    "        encoding, encoder_states = self.encode(phrase)\n",
    "        gen_output = self.generate(encoding, encoder_states, sim_phrase if train else None, train)\n",
    "        emb_gt = self.compute_embeddings(sim_phrase)\n",
    "        emb_gen = self.compute_embeddings_from_logits(gen_output)\n",
    "        return gen_output, emb_gen.squeeze(0), emb_gt.squeeze(0)\n",
    "\n",
    "    def compute_embeddings(self, sequence):\n",
    "        embedded = self.emb_layer(sequence)\n",
    "        _, hidden_state = self.enc_rnn(embedded)\n",
    "        num_layers = hidden_state.size(0) // 2\n",
    "        forward_hidden = hidden_state[num_layers-1]\n",
    "        backward_hidden = hidden_state[-1]\n",
    "        combined_hidden = torch.cat([forward_hidden, backward_hidden], dim=-1)\n",
    "        return self.enc_lin(combined_hidden).unsqueeze(0)\n",
    "\n",
    "    def compute_embeddings_from_logits(self, logits):\n",
    "        # Pad the generated sequence to the expected length before embedding\n",
    "        # This is needed because the generation process can stop early.\n",
    "        seq_len, batch_size, vocab_sz = logits.shape\n",
    "        padded_logits = torch.full((self.max_seq_len, batch_size, vocab_sz), fill_value=-1e9, dtype=torch.float, device=logits.device)\n",
    "        padded_logits[:seq_len, :, :] = logits\n",
    "        \n",
    "        predicted_tokens = self.prob2pred(padded_logits)\n",
    "        \n",
    "        # Check for sequences that are all padding after early stopping\n",
    "        if predicted_tokens.numel() == 0:\n",
    "            return torch.zeros(1, batch_size, self.enc_dim, device=logits.device)\n",
    "        \n",
    "        embedded = self.emb_layer(predicted_tokens)\n",
    "        _, hidden_state = self.enc_rnn(embedded)\n",
    "        num_layers = hidden_state.size(0) // 2\n",
    "        forward_hidden = hidden_state[num_layers-1]\n",
    "        backward_hidden = hidden_state[-1]\n",
    "        combined_hidden = torch.cat([forward_hidden, backward_hidden], dim=-1)\n",
    "        return self.enc_lin(combined_hidden).unsqueeze(0)\n",
    "\n",
    "    def prob2pred(self, logits):\n",
    "        return torch.argmax(logits, dim=-1)\n",
    "\n",
    "    def should_stop(self, logits, eos_token_id):\n",
    "        predicted = self.prob2pred(logits)\n",
    "        return (predicted == eos_token_id).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e702fa27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T17:08:36.439032Z",
     "iopub.status.busy": "2025-08-01T17:08:36.438517Z",
     "iopub.status.idle": "2025-08-01T17:08:36.443214Z",
     "shell.execute_reply": "2025-08-01T17:08:36.442433Z"
    },
    "papermill": {
     "duration": 0.009658,
     "end_time": "2025-08-01T17:08:36.444387",
     "exception": false,
     "start_time": "2025-08-01T17:08:36.434729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# token-level cross-entropy\n",
    "ce_loss = nn.CrossEntropyLoss(ignore_index=dataset.PAD_token)\n",
    "\n",
    "# pair-wise discriminator margin loss\n",
    "def JointEmbeddingLoss(emb1, emb2, margin=1.0):\n",
    "    sims = emb1 @ emb2.t()\n",
    "    pos  = sims.diag().unsqueeze(1)\n",
    "    raw  = sims - pos + margin\n",
    "    raw.fill_diagonal_(0)\n",
    "    return torch.clamp(raw, min=0).sum() / (emb1.size(0)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd8828c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T17:08:36.452644Z",
     "iopub.status.busy": "2025-08-01T17:08:36.452213Z",
     "iopub.status.idle": "2025-08-01T17:08:36.476516Z",
     "shell.execute_reply": "2025-08-01T17:08:36.475743Z"
    },
    "papermill": {
     "duration": 0.029945,
     "end_time": "2025-08-01T17:08:36.477769",
     "exception": false,
     "start_time": "2025-08-01T17:08:36.447824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def vec_norm(seq, n=4):\n",
    "    tokens = seq.split()\n",
    "    ngrams = [{} for _ in range(n)]\n",
    "    norms = [0.0] * n\n",
    "    for i in range(n):\n",
    "        for j in range(len(tokens) - i):\n",
    "            ngram = tuple(tokens[j:j+i+1])\n",
    "            ngrams[i][ngram] = ngrams[i].get(ngram, 0) + 1\n",
    "            norms[i] += ngrams[i][ngram]\n",
    "        norms[i] = norms[i] ** 0.5 if norms[i] > 0 else 1.0\n",
    "    return ngrams, norms\n",
    "\n",
    "def get_ngrams(seq, n):\n",
    "    tokens = seq.split()\n",
    "    return Counter(tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1))\n",
    "\n",
    "# BLEU\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def get_ngrams(seq, n):\n",
    "    \"\"\"\n",
    "    Extract n-grams from a sequence of tokens.\n",
    "    \n",
    "    Args:\n",
    "        seq (str): Space-separated sequence of tokens.\n",
    "        n (int): N-gram order.\n",
    "    \n",
    "    Returns:\n",
    "        Counter: Dictionary of n-grams and their counts.\n",
    "    \"\"\"\n",
    "    tokens = seq.split()\n",
    "    return Counter(tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1))\n",
    "\n",
    "class BleuScorer:\n",
    "    def __init__(self, n=4):\n",
    "        self.n = n\n",
    "        self.hyp = []\n",
    "        self.refs = []\n",
    "    \n",
    "    def add_instance(self, hyp, refs):\n",
    "        \"\"\"\n",
    "        Add a hypothesis and its reference(s).\n",
    "        \n",
    "        Args:\n",
    "            hyp (str): Hypothesis sequence.\n",
    "            refs (list): List of reference sequences.\n",
    "        \"\"\"\n",
    "        self.hyp.append(hyp)\n",
    "        self.refs.append(refs)\n",
    "    \n",
    "    def compute_score(self):\n",
    "        \"\"\"\n",
    "        Compute BLEU scores for n-grams (1 to n).\n",
    "        \n",
    "        Returns:\n",
    "            scores (list): BLEU scores for each n-gram order.\n",
    "            None: Placeholder for compatibility.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for n in range(1, self.n + 1):\n",
    "            total_precision = 0.0\n",
    "            total_brevity_penalty = 0.0\n",
    "            count = 0\n",
    "            \n",
    "            for hyp, refs in zip(self.hyp, self.refs):\n",
    "                hyp_ngrams = get_ngrams(hyp, n)\n",
    "                hyp_len = max(len(hyp.split()) - n + 1, 1)\n",
    "                \n",
    "                # Compute clipped counts\n",
    "                ref_ngrams_list = [get_ngrams(ref, n) for ref in refs]\n",
    "                ref_counts = Counter()\n",
    "                for ref_ngrams in ref_ngrams_list:\n",
    "                    for ng in ref_ngrams:\n",
    "                        ref_counts[ng] = max(ref_counts[ng], ref_ngrams[ng])\n",
    "                \n",
    "                clipped_count = sum(min(hyp_ngrams[ng], ref_counts.get(ng, 0)) for ng in hyp_ngrams)\n",
    "                precision = clipped_count / hyp_len if hyp_len > 0 else 0.0\n",
    "                \n",
    "                # Brevity penalty\n",
    "                hyp_words = len(hyp.split())\n",
    "                ref_lengths = [len(ref.split()) for ref in refs]\n",
    "                closest_ref_len = min(ref_lengths, key=lambda x: abs(x - hyp_words))\n",
    "                brevity_penalty = 1.0 if hyp_words >= closest_ref_len else math.exp(1 - closest_ref_len / hyp_words if hyp_words > 0 else 0)\n",
    "                \n",
    "                total_precision += precision\n",
    "                total_brevity_penalty += brevity_penalty\n",
    "                count += 1\n",
    "            \n",
    "            avg_precision = total_precision / count if count > 0 else 0.0\n",
    "            avg_brevity_penalty = total_brevity_penalty / count if count > 0 else 1.0\n",
    "            bleu_n = avg_precision * avg_brevity_penalty if avg_precision > 0 else 0.0\n",
    "            scores.append(bleu_n)\n",
    "        \n",
    "        return scores, None\n",
    "\n",
    "class Bleu:\n",
    "    def __init__(self, n=4):\n",
    "        self.n = n\n",
    "        self.scorer = BleuScorer(n=n)\n",
    "    \n",
    "    def compute_score(self, gts, res):\n",
    "        \"\"\"\n",
    "        Compute BLEU score for ground truth and predicted sequences.\n",
    "        \n",
    "        Args:\n",
    "            gts (dict): {id: [ref_str, ...]} mapping IDs to lists of reference strings.\n",
    "            res (dict): {id: [hyp_str]} mapping IDs to lists of hypothesis strings.\n",
    "        \n",
    "        Returns:\n",
    "            scores (list): BLEU scores for n-grams (1 to n).\n",
    "            None: Placeholder for compatibility.\n",
    "        \"\"\"\n",
    "        for id in gts:\n",
    "            self.scorer.add_instance(res[id][0], gts[id])\n",
    "        return self.scorer.compute_score()\n",
    "\n",
    "# CIDEr\n",
    "def vec_norm(seq, n=4):\n",
    "    \"\"\"\n",
    "    Convert a sequence (string) into a list of n-gram count dictionaries and their norms.\n",
    "    \n",
    "    Args:\n",
    "        seq (str): Space-separated sequence of tokens (e.g., \"how to learn python\").\n",
    "        n (int): Maximum n-gram order (e.g., 4 for 1- to 4-grams).\n",
    "    \n",
    "    Returns:\n",
    "        ngrams (list): List of dictionaries, where ngrams[i] maps (i+1)-grams to counts.\n",
    "        norms (list): List of norms for each n-gram order.\n",
    "    \"\"\"\n",
    "    # Split string into tokens\n",
    "    tokens = seq.split()\n",
    "    ngrams = [{} for _ in range(n)]\n",
    "    norms = [0.0] * n\n",
    "    \n",
    "    # Generate n-grams for each order (1 to n)\n",
    "    for i in range(n):\n",
    "        for j in range(len(tokens) - i):\n",
    "            ngram = tuple(tokens[j:j+i+1])\n",
    "            ngrams[i][ngram] = ngrams[i].get(ngram, 0) + 1\n",
    "            norms[i] += ngrams[i][ngram]\n",
    "        norms[i] = norms[i] ** 0.5 if norms[i] > 0 else 1.0\n",
    "    \n",
    "    return ngrams, norms\n",
    "\n",
    "class CiderScorer:\n",
    "    def __init__(self, n=4):\n",
    "        self.n = n\n",
    "        self.hyp = []\n",
    "        self.refs = []\n",
    "    \n",
    "    def add_instance(self, hyp, refs):\n",
    "        \"\"\"\n",
    "        Add a hypothesis and its reference(s) to the scorer.\n",
    "        \n",
    "        Args:\n",
    "            hyp (str): Hypothesis sequence (e.g., \"how to study python\").\n",
    "            refs (list): List of reference sequences (e.g., [\"how to learn python\"]).\n",
    "        \"\"\"\n",
    "        self.hyp.append(hyp)\n",
    "        self.refs.append(refs)\n",
    "    \n",
    "    def compute_score(self):\n",
    "        \"\"\"\n",
    "        Compute CIDEr scores for all n-grams up to self.n.\n",
    "        \n",
    "        Returns:\n",
    "            scores (list): CIDEr scores for n-grams (1 to n).\n",
    "            None: Placeholder for compatibility.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for i in range(self.n):\n",
    "            num = 0.0\n",
    "            for hyp, refs in zip(self.hyp, self.refs):\n",
    "                vh, nh = vec_norm(hyp, i+1)\n",
    "                for ref in refs:\n",
    "                    vr, nr = vec_norm(ref, i+1)\n",
    "                    num += sum(min(vh[i].get(ng, 0), vr[i].get(ng, 0)) * vr[i].get(ng, 0)\n",
    "                               for ng in set(vh[i]) | set(vr[i]))\n",
    "                    den = nh[i] * nr[i] if nh[i] and nr[i] else 1.0\n",
    "                    num /= den\n",
    "            scores.append(num / len(self.hyp) if self.hyp else 0.0)\n",
    "        return scores, None\n",
    "\n",
    "class Cider:\n",
    "    def __init__(self, n=4):\n",
    "        self.n = n\n",
    "        self.scorer = CiderScorer(n=n)\n",
    "    \n",
    "    def compute_score(self, gts, res):\n",
    "        \"\"\"\n",
    "        Compute CIDEr score for ground truth (gts) and predicted (res) sequences.\n",
    "        \n",
    "        Args:\n",
    "            gts (dict): {id: [ref_str, ...]} mapping IDs to lists of reference strings.\n",
    "            res (dict): {id: [hyp_str]} mapping IDs to lists of hypothesis strings.\n",
    "        \n",
    "        Returns:\n",
    "            score (float): Aggregated CIDEr score.\n",
    "            scores (list): CIDEr scores for each n-gram order.\n",
    "        \"\"\"\n",
    "        for id in gts:\n",
    "            self.scorer.add_instance(res[id][0], gts[id])\n",
    "        scores, _ = self.scorer.compute_score()\n",
    "        return sum(scores) / len(scores) if scores else 0.0, scores\n",
    "  \n",
    "\n",
    "# ROUGE-L\n",
    "class Rouge:\n",
    "    def __init__(self,beta=1.2): self.beta=beta\n",
    "    def _lcs(self,x,y):\n",
    "        m,n=len(x),len(y); dp=[[0]*(n+1) for _ in range(m+1)]\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                dp[i+1][j+1] = dp[i][j]+1 if x[i]==y[j] else max(dp[i][j+1],dp[i+1][j])\n",
    "        return dp[m][n]\n",
    "    def compute_score(self,gts,res):\n",
    "        scores=[]\n",
    "        for id in gts:\n",
    "            h=res[id][0].split(); refs=[r.split() for r in gts[id]]\n",
    "            prec=[self._lcs(r,h)/len(h) for r in refs]\n",
    "            rec=[self._lcs(r,h)/len(r) for r in refs]\n",
    "            pmax, rmax = max(prec), max(rec)\n",
    "            scores.append((1+self.beta**2)*pmax*rmax/(rmax+self.beta**2*pmax) if pmax and rmax else 0.0)\n",
    "        return float(np.mean(scores)), np.array(scores)\n",
    "class RougeL(Rouge): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa657b8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T17:08:36.485736Z",
     "iopub.status.busy": "2025-08-01T17:08:36.485464Z",
     "iopub.status.idle": "2025-08-01T17:28:28.383067Z",
     "shell.execute_reply": "2025-08-01T17:28:28.381917Z"
    },
    "papermill": {
     "duration": 1191.906258,
     "end_time": "2025-08-01T17:28:28.387455",
     "exception": false,
     "start_time": "2025-08-01T17:08:36.481197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-01 17:08:38.273155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754068118.471809      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754068118.526629      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /kaggle/input/qqp-processed/quora_data_prepro.json\n",
      "Loading HDF5 from /kaggle/input/qqp-processed/quora_data_prepro.h5\n",
      "Training samples: 100000\n",
      "Test samples: 30000\n",
      "ques shape: torch.Size([130000, 28])\n",
      "label shape: torch.Size([130000, 28])\n",
      "len shape: torch.Size([130000])\n",
      "label_len shape: torch.Size([130000])\n",
      "id shape: torch.Size([130000])\n",
      "Epoch 1 | Train L1=16.7918 L2=4.1549 | Val L1=7.3544 L2=1.0036 | BLEU-4=0.0000 CIDEr=0.0000 ROUGE-L=0.2802\n",
      "Epoch 2 | Train L1=5.4749 L2=0.6300 | Val L1=7.7335 L2=0.9324 | BLEU-4=0.0000 CIDEr=0.0000 ROUGE-L=0.3166\n",
      "Epoch 3 | Train L1=4.7405 L2=0.1871 | Val L1=6.0882 L2=0.7836 | BLEU-4=0.0106 CIDEr=0.0000 ROUGE-L=0.3244\n",
      "Epoch 4 | Train L1=3.8206 L2=0.0373 | Val L1=5.3556 L2=0.4418 | BLEU-4=0.0268 CIDEr=0.0000 ROUGE-L=0.3456\n",
      "Epoch 5 | Train L1=3.2947 L2=0.0214 | Val L1=5.6532 L2=0.4426 | BLEU-4=0.0537 CIDEr=0.0000 ROUGE-L=0.4442\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "JSON_PATH = '/kaggle/input/qqp-processed/quora_data_prepro.json'\n",
    "H5_PATH = '/kaggle/input/qqp-processed/quora_data_prepro.h5'\n",
    "LOG_DIR = '/kaggle/working/logs'\n",
    "SAVE_DIR = '/kaggle/working/save'\n",
    "SAMPLE_DIR = '/kaggle/working/samples'\n",
    "\n",
    "for d in (LOG_DIR, SAVE_DIR, SAMPLE_DIR): os.makedirs(d, exist_ok=True)\n",
    "\n",
    "TIME = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparams\n",
    "BATCH_SIZE = 364\n",
    "NUM_EPOCHS = 5\n",
    "TRAIN_SIZE = 100000\n",
    "VAL_SIZE = 30000\n",
    "LR = 1e-3\n",
    "\n",
    "# Data\n",
    "dataset = Dataloader(JSON_PATH, H5_PATH)\n",
    "train_ds = Subset(dataset, range(TRAIN_SIZE))\n",
    "val_ds = Subset(dataset, range(TRAIN_SIZE, TRAIN_SIZE+VAL_SIZE))\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, drop_last=True)\n",
    "\n",
    "# Model & optimizer\n",
    "op = {\"vocab_sz\": dataset.vocab_size,\n",
    "      \"emb_hid_dim\": 256, \"emb_dim\": 512,\n",
    "      \"enc_rnn_dim\": 512, \"enc_dim\": 512, \"enc_dropout\": 0.5,\n",
    "      \"gen_rnn_dim\": 512, \"gen_dropout\": 0.5,\n",
    "      \"max_seq_len\": dataset.getSeqLength(), # Using the getter method\n",
    "      \"SOS_token\": dataset.SOS_token,\n",
    "      \"EOS_token\": dataset.EOS_token,\n",
    "      \"PAD_token\": dataset.PAD_token}\n",
    "model = ParaphraseGenerator(op).to(DEVICE)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=LR)\n",
    "ce_loss = nn.CrossEntropyLoss(ignore_index=dataset.PAD_token)\n",
    "\n",
    "# Logger\n",
    "writer = SummaryWriter(os.path.join(LOG_DIR, TIME))\n",
    "\n",
    "def dump_samples(ins, gts, preds, fname):\n",
    "    with open(fname, 'w') as f:\n",
    "        for i, g, p in zip(ins, gts, preds):\n",
    "            f.write(f\"IN : {i}\\nGT : {g}\\nPR : {p}\\n---\\n\")\n",
    "\n",
    "# Training\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    # Train\n",
    "    model.train()\n",
    "    sum_l1 = 0\n",
    "    sum_l2 = 0\n",
    "    cnt = 0\n",
    "    all_in, all_gt, all_pr = [], [], []\n",
    "    for inp, il, lbl, ll, ids in train_loader:\n",
    "        inp, lbl = inp.t().to(DEVICE), lbl.t().to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out, eg, et = model(inp, sim_phrase=lbl, train=True)\n",
    "        \n",
    "        # Fix: The model generates output for target_seq[:-1], so we need to align with lbl[1:]\n",
    "        # out shape: [seq_len-1, batch_size, vocab_sz] - predictions for positions 1 to seq_len-1\n",
    "        # lbl[1:] shape: [seq_len-1, batch_size] - target tokens for positions 1 to seq_len-1\n",
    "        target_tokens = lbl[1:]  # Skip SOS token, these are the tokens we want to predict\n",
    "        \n",
    "        # Reshape for cross entropy: [batch_size, vocab_sz, seq_len] and [batch_size, seq_len]\n",
    "        l1 = ce_loss(out.permute(1, 2, 0), target_tokens.t())\n",
    "        l2 = JointEmbeddingLoss(eg, et)\n",
    "        (l1+l2).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        sum_l1 += l1.item()\n",
    "        sum_l2 += l2.item()\n",
    "        cnt += 1\n",
    "        all_in += decode_sequence(dataset.ix_to_word, inp.t().cpu())\n",
    "        all_gt += decode_sequence(dataset.ix_to_word, lbl.t().cpu())\n",
    "        all_pr += decode_sequence(dataset.ix_to_word, torch.argmax(out, dim=-1).t().cpu())\n",
    "    writer.add_scalar('L1/train', sum_l1/cnt, epoch)\n",
    "    writer.add_scalar('L2/train', sum_l2/cnt, epoch)\n",
    "    dump_samples(all_in[:5], all_gt[:5], all_pr[:5], f\"{SAMPLE_DIR}/{TIME}_train{epoch}.txt\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    v1 = 0\n",
    "    v2 = 0\n",
    "    vc = 0\n",
    "    vin, vgt, vpr = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for inp, il, lbl, ll, ids in val_loader:\n",
    "            inp, lbl = inp.t().to(DEVICE), lbl.t().to(DEVICE)\n",
    "            out, eg, et = model(inp, sim_phrase=lbl, train=False)\n",
    "            \n",
    "            # For validation, we need to handle the case where generation might be shorter\n",
    "            # than the target sequence due to early stopping\n",
    "            if out.size(0) > 0:  # Check if any output was generated\n",
    "                # Truncate target to match output length if needed\n",
    "                target_len = min(out.size(0), lbl.size(0) - 1)\n",
    "                out_truncated = out[:target_len]\n",
    "                target_truncated = lbl[1:target_len+1]\n",
    "                \n",
    "                if target_truncated.size(0) > 0:\n",
    "                    l1 = ce_loss(out_truncated.permute(1, 2, 0), target_truncated.t())\n",
    "                else:\n",
    "                    l1 = torch.tensor(0.0, device=DEVICE)\n",
    "            else:\n",
    "                l1 = torch.tensor(0.0, device=DEVICE)\n",
    "                \n",
    "            l2 = JointEmbeddingLoss(eg, et)\n",
    "            v1 += l1.item()\n",
    "            v2 += l2.item()\n",
    "            vc += 1\n",
    "            vin += decode_sequence(dataset.ix_to_word, inp.t().cpu())\n",
    "            vgt += decode_sequence(dataset.ix_to_word, lbl.t().cpu())\n",
    "            if out.size(0) > 0:\n",
    "                vpr += decode_sequence(dataset.ix_to_word, torch.argmax(out, dim=-1).t().cpu())\n",
    "            else:\n",
    "                # Handle case where no output was generated\n",
    "                vpr += [''] * inp.size(0)\n",
    "    writer.add_scalar('L1/val', v1/vc, epoch)\n",
    "    writer.add_scalar('L2/val', v2/vc, epoch)\n",
    "\n",
    "    # Metrics calculation\n",
    "    bleu = Bleu(n=4)\n",
    "    cider = Cider(n=4)\n",
    "    rouge = RougeL()\n",
    "    gts = {i: [gt] for i, gt in enumerate(vgt)}\n",
    "    res = {i: [pr] for i, pr in enumerate(vpr)}\n",
    "    \n",
    "    # Only compute metrics if we have valid predictions\n",
    "    if any(pred.strip() for pred in vpr):\n",
    "        bleu_score = bleu.compute_score(gts, res)[0]\n",
    "        cider_score = cider.compute_score(gts, res)[0]\n",
    "        rouge_score = rouge.compute_score(gts, res)[0]\n",
    "        writer.add_scalar('BLEU/val', bleu_score[-1], epoch)\n",
    "        writer.add_scalar('CIDEr/val', cider_score, epoch)\n",
    "        writer.add_scalar('ROUGE-L/val', rouge_score, epoch)\n",
    "    else:\n",
    "        bleu_score = [0, 0, 0, 0]\n",
    "        cider_score = 0\n",
    "        rouge_score = 0\n",
    "    \n",
    "    dump_samples(vin[:5], vgt[:5], vpr[:5], f\"{SAMPLE_DIR}/{TIME}_val{epoch}.txt\")\n",
    "\n",
    "    # Checkpoint\n",
    "    os.makedirs(f\"{SAVE_DIR}/{TIME}\", exist_ok=True)\n",
    "    torch.save({'epoch': epoch, 'model': model.state_dict(), 'opt': optimizer.state_dict()},\n",
    "               f\"{SAVE_DIR}/{TIME}/epoch{epoch}.pt\")\n",
    "    print(f\"Epoch {epoch} | Train L1={sum_l1/cnt:.4f} L2={sum_l2/cnt:.4f} | Val L1={v1/vc:.4f} L2={v2/vc:.4f} | BLEU-4={bleu_score[-1]:.4f} CIDEr={cider_score:.4f} ROUGE-L={rouge_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d8bc5",
   "metadata": {
    "papermill": {
     "duration": 0.003695,
     "end_time": "2025-08-01T17:28:28.395164",
     "exception": false,
     "start_time": "2025-08-01T17:28:28.391469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 747,
     "sourceId": 1423,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7983962,
     "sourceId": 12634958,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1242.917249,
   "end_time": "2025-08-01T17:28:31.443090",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-01T17:07:48.525841",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
